---
tags:
  - category/seminar
  - topic/
date: 16-02-2025 17:38:58
lecturer: Gianfranco Bilardi
---
# Transformers - Argos, Babele e il Lego della Vita
## Concetti
- proposto come modello nel 2017
- un **transformer** cattura un _modello stocastico del linguaggio_: permette quindi di calcolare la probabilità della parola successiva
- è il primo modello in grado di farlo per bene, e non solo: _il testo generato stocasticamente è anche sensato_
- è _estremamente flessibile_, per esempio è stato usato per AlphaFold
- **machine learning**
	- disciplina per automatizzare lo sviluppo di modelli scientifici
	- in realtà la scienza ha sempre seguito i passi tipici del machine learning, esempio le leggi di Keplero --> sono state calcolate sulla base di dati raccolti, e si fa l'inferenza per sapere la posizione dei pianeti nel futuro
- l'autocompletamento è una forma rudimentale di previsione della parola successiva
- **language model**
	- ingredienti:
		- _dizionario_ di _token_
		- modello stesso
	- le probabilità crescono esponenzialmente per la certa di un $t$ esimo token
		- si possono usare le catene di Markov, ma il linguaggio non si modella per bene in questo modo

## Domande

## Referenze