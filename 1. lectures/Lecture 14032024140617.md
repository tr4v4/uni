---
tags:
  - category/lecture
  - status/pending
  - topic/analisi-I
date: 14-03-2024 14:06:17
teacher: daniele.morbidelli@unibo.it
mod: 2
---
# Lezione
---
## Concetti
- dobbiamo recuperare le ore di analisi perse
- richiami della lezione scorsa
	- [[Prodotto scalare|prodotto scalare]]
	- norma
	- coordinate polari in $\mathbb{R}^{2}$
	- Cauchy-Schwarz
	- "quadrato di un binomio" di vettori (anche detto teorema del coseno/teorema di Carnot, che è una generalizzazione del teorema di Pitagora per il caso $x \cdot y = 0$)
- disuguaglianza triangolare
	- proposizione: $\forall x, y \in \mathbb{R}^{n} \ \ \ ||x + y|| \leq ||x|| + ||y||$
	- dimostrazione: dimostriamo che $||x + y||^{2} \leq (||x|| + ||y||)^{2}$
		- allora calcoliamo $||x + y||^{2} = ||x||^{2} + 2xy + ||y||^{2} \leq ||x||^{2} + ||y||^{2} + 2||xy||$
		- ora per Cauchy-Schwarz ho che $||x||^{2} + ||y||^{2} + 2||xy|| \leq ||x||^{2} + ||y||^{2} + 2||x|| \cdot ||y|| = (||x|| + ||y||)^{2}$
		- per cui ho $||x+y||^{2} \leq (||x|| + ||y||)^{2}$, e togliendo i quadrati ottengo la proposizione
- introduciamo la distanza tra punti
	- definizione: dati $x, y \in \mathbb{R}^{n}$, la distanza tra $x$ e $y$ è $||x-y||$
	- utilizzi: con la distanza si fanno gli [[Intorno|intorni]], con cui si fanno i [[Limite|limiti]], perciò è importante
	- esempio: $x=(x_{1}, x_{2}) \in \mathbb{R}^{2}, y = (y_{1}, y_{2}) \in \mathbb{R}^{2}$, se faccio $||x-y|| = ||(x_{1}-y_{1}, x_{2}-y_{2})|| = \sqrt{(x_{1}-x_{2})^{2} + (y_{1}-y_{2})^{2}}$
	- altro esempio: $x, y \in \mathbb{R}^{n}$ ho $||x-y|| = \sqrt{\sum\limits_{k=1}^{n} (x_{k}-y_{k})^{2}} = \sqrt{(x_{1}-y_{1})^{2} + \cdots + (x_{n}-y_{n})^{2}}$
	- notazione: $||x-y|| \equiv \text{dist}(x, y)$
	- esempio numerico
	- useremo la formula della distanza anche per il [[Metodo dei minimi quadrati|metodo dei minimi quadrati]], per risolvere [[Sistema lineare|sistemi lineari]]
	- punto di minima distanza da una retta, esempio
		- siamo in $\mathbb{R}^{n}, v \neq 0, x \in \mathbb{R}^{n}$, e consideriamo la linea seguente $l_{v} = \{\lambda \cdot {v} | \lambda \in \mathbb{R}\}$ ([[Sottospazi vettoriali di R2|sottospazio di R2]]), fa una retta
		- ora prendiamo il punto $x$
		- cerchiamo tra tutti i punti di $l_{v}$ quello che ha minima distanza da $x$
			- di fatto devo minimizzare la funzione $h: \mathbb{R} \to \mathbb{R}, h(\lambda) = |x - \lambda v| =$ distanza tra $x$ e $\lambda v$
		- considero la funzione $g(\lambda) = h(\lambda)^{2} = |x - \lambda v|^{2}$
		- calcolo $g(\lambda) = |x|^{2} + |\lambda v|^{2} + 2(x \cdot (- \lambda v)) = |x|^{2} + \lambda^{2}|v|^{2} - 2 \lambda(x \cdot v)$, che è una parabola concavità in alto
			- $g'(\lambda) = 2\lambda |v|^{2} - 2xv = 0 \iff \underline{\lambda} = \frac{xv}{|v|^{2}}$, che è l'unico punto di minimo (assoluto, perché equazione di secondo grado, appunto parabola)
			- quindi il punto di $l_{v}$ che minimizza la distanza è $\underline{\lambda} v = \frac{xv}{|v|^{2}} \cdot v \in l_{v}$
		- corrispondentemente, la distanza è data da $$\text{dist}(x, \underline{\lambda}v)^{2} = g(\underline{\lambda}) = |x - \frac{xv}{|v|^{2}}v|^{2} = |x|^{2} + \frac{(xv)^{2}}{|v|^{4}} |v|^{2} - 2x \frac{xv}{|v|^{2}}v = |x|^{2} + \frac{(xv)^{2}}{|v|^{2}} - 2\frac{xv}{|v|^{2}}xv = |x|^{2} - \frac{(xv)^{2}}{|v|^{2}}$$
		- per cui $$\text{dist}(x, \underline{\lambda}v) = \sqrt{|x|^{2} - \frac{(xv)^{2}}{|v|^{2}}}$$
			- esempio
- proprietà di ortogonalità
	- proposizione: dati $v \neq 0$ e $x \in \mathbb{R}^{n}$, il punto di minima distanza$\frac{xv}{|v|^{2}}v$ soddisfa $(x - \frac{xv}{|v|^{2}}v)v = 0 \iff xv - \frac{xv}{|v|^{2}}v \cdot v = xv - xv = 0$
- intorno
	- $D(x, r) = \{y \in \mathbb{R}^{n} : |x-y| < r\}$
	- disco di centro $x \in \mathbb{R}^{n}$ e reggio $r > 0$
	- esempi su varie dimensioni e su centri non per forza sull'origine
- insiemi aperti
	- figure a cui hai tolto la buccia
	- definizione: $A \subseteq \mathbb{R}^{2}$ si dice aperto se $\forall x \in A, \exists \epsilon > 0 : D(x, \epsilon) \subseteq A$
- successioni in $\mathbb{R}^{2}$
	- definizione: una successione $(x_{k})_{k \in \mathbb{N}}$ è un vettore di $n$ successioni: $$x_{k} = (x_{k}^{(1)}, \cdots, x_{k}^{(n)}) \ \ \ \ \ \text{con } k \in \mathbb{N}$$
	- definizione di successione convergente: sia $(x_{k})_{k \in \mathbb{N}}$ una successione in $\mathbb{R}^{n}$, e sia $x \in \mathbb{R}^{n}$, allora si dice che $\lim_{k \to +\infty} x_{k} = x$ se risulta $\lim_{k \to +\infty} x_{k}^{(1)} \longrightarrow x^{(1)}$, $\cdots$, $\lim_{k \to +\infty} x_{k}^{(n)} \longrightarrow x^{(n)}$
	- si dimostra che $(x_{k})_{k \in \mathbb{N}}$ in $\mathbb{R}^{n}$, allora $\lim_{k \to +\infty} x_{k} = x \in \mathbb{R}^{n} \iff \lim_{k \to +\infty} |x_{k} - x| = 0$
- limite

## Domande

## Referenze