---
tags:
  - category/lecture
  - status/pending
  - topic/calcolo-delle-probabilità-e-statistica
date: 06-05-2025 13:30:22
teacher: elena.bandini7@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- andiamo a studiare successioni di variabili aleatorie
- consideriamo fondamentalmente vettori aleatori infiniti, ossia con infiniti elementi
- avrò fondamentalmente 2 situazioni: ho l'indipendenza o non ce l'ho
	- se sono indipendenti hanno un ruolo enorme --> in contesti in cui idealmente $n \to +\infty$ e le variabili aleatorie sono indipendenti, _valgono i teoremi limite_
		- ci dicono cosa succede in questo caso
		- teorema limite centrale
			- se prendo la media (non pesata) e mando $n$ a $\infty$, questa nuova variabile aleatoria converge alla variabile aleatoria singola...
- problema fondamentale della statistica: esperimento aleatorio, $X$ v.a. legata all'esperimento, ci chiediamo $\mathbb{P}_{X}$, ossia voglio stimare la legge di $X$
	- idea: per stimarne la legge devo stimarne il valore atteso
		- perché è buono --> se so stimare $\mathbb{E}[X]$, so stimare $\mathbb{E}[f(x)]$ $\forall f: \mathbb{R} \to \mathbb{R}$, e questo equivale a stimare la legge
		- inoltre, se so stimare $\mathbb{E}[X]$ con $X$ v.a. generica, io so stimare $\mathbb{P}(A)$ (basta prendere $X = \mathbb{1}_{A}$, infatti diventa $\mathbb{E}[X] = \mathbb{P}(A)$)
	- ma come si stima il valore atteso?
		- ricorda: facciamo una media pesata
		- idea: si può approssimare con la media numerica, ossia la media non pesata!
		- formalmente stimo $\mathbb{E}[X]$ con $\bar{x}_{n} : \frac{x_{1} + \cdots + x_{n}}{n}$ dove $x_{1}, \cdots, x_{n}$ sono i valori realizzati da $X$ quando ripeto tante volte l'esperimento
		- dal punto di vista teorico ho delle variabili aleatorie $X_{1}, X_{2}, \cdots, X_{n}, \cdots$ (teoricamente infinite) di cui $x_{1}, x_{2}, \cdots, x_{n}, \cdots$ sono i valori noti una volta che ho eseguito l'esperimento
			- l'ipotesi è quindi che ho $(X_{n})_{n \in \mathbb{N}}$ successione di variabili aleatorie dette I.I.D., ovvero Indipendenti e Identicamente Distribuite
			- voglio mostrare che $\bar{x}_{n}$ è una buona stima di $\mathbb{E}[X]$
			- il teorema che dice questa cosa è: _legge dei grandi numeri_ (LGN)
				- sia $(X_{n})_{n \in \mathbb{N}}$ una successione di v.a. I.I.D., allora definisco $\bar{X}_{n} : \frac{X_{1} + \cdots + X_{n}}{n}$
				- fisso $\mu = \mathbb{E}[X]$ e $\sigma^{2} = Var(X)$
				- mostro che $\mathbb{P}(|\bar{X}_{n} - \mu| > \epsilon) \leq \frac{\sigma^{2}}{n\epsilon^{2}} \ \ \ \forall \epsilon > 0$
				- quindi in particolare che $$\lim_{n \to +\infty} \mathbb{P}(|\bar{X}_{n} - \mu| > \epsilon) = 0 \ \ \ \forall \epsilon > 0$$
					- e quindi $$\bar{X}_{n} \to_{n \to +\infty}^{\mathbb{P}} \mu$$
					- ossia che $\bar{X}_{n}$ converga in probabilità a $\mu$
				- dimostrazione
					- serve il lemma della disuguaglianza di Chebychev:
						- $Y$ v.a. con $\mathbb{E}[Y]$ e $Var(Y)$ conosciuti, allora $$\forall \epsilon > 0 \ \ \ \mathbb{P}(|Y - \mathbb{E}[Y]| > \epsilon) \leq \frac{Var(Y)}{\epsilon^{2}}$$
						- dimostrazione
							- $Var(Y) = \mathbb{E}[(Y - \mathbb{E}[Y])^{2}]$
							- ...
					- a sto punto basta usare il lemma, e prendere $Y = \bar{X}_{n}$, e sapendo che $\bar{X}_{n} = \frac{1}{n} \sum\limits_{i=1}^{n} X_{i}$ allora $$\mathbb{E}[\bar{X}_{n}] = \frac{1}{n} \sum\limits_{i=1}^{n} \mathbb{E}[X_{i}] = \frac{n\mu}{n} = \mu$$
					- ora dimostro che $Var(\bar{X}_{n}) = \frac{\sigma^{2}}{n}$
						- si usa il fatto che $(X_{n})_{n}$ sono identicamente distribuite con $Var(X_{i}) = \sigma^{2}$, e in più che sono indipendenti
						- dimostrazione
					- una volta fatto la dimostrazione è ovvia, dal lemma di Chebychev
	- _metodo monte carlo_, come applicazione della LGN
		- obbiettivo: approssimare degli integrali generici, ossia $\int_{a}^{b} f(x) \, dx$
		- ipotesi: $a = 0, b = 1$, allora $\int_{0}^{1} f(x) \, dx = \int_{\mathbb{R}} f(x) \mathbb{1}_{(0,1)}(x) \, dx = \mathbb{E}[f(X)]$
			- allora $\mathbb{1}_{(0,1)}(x) = f_{X}(x)$, è la funzione di densità continua
			- quindi $X \sim Unif(0, 1)$
			- prendo $y_{i} = f(X_{i})$ con $X_{i}$ uniforme su $(0, 1)$
			- considero $(X_{n})_{n}$ successione di v.a. i.i.d.
			- e so che $\sum\limits_{i=1}^{n}y_{i} = \frac{f(X_{1}), \cdots, f(X_{n})}{n} \to_{n \to +\infty}^{\mathbb{P}} \mathbb{E}[f(X)]$ che è uguale a $\int_{0}^{1} f(x) \, dx$
			- la convergenza è dell'ordine di $\frac{1}{\sqrt{n}}$, e non dipende dalla dimensione dell'integrale
				- questo significa che $\mathbb{E}[|\bar{X}_{n} - \mu|] \simeq \frac{1}{\sqrt{n}}$
				- è una conseguenza del _teorema limite centrale_ (TLC), o teorema centrale del limite
					- $(X_{n})_{n}$ v.a. i.i.d., con media $\mu$ e varianza $\sigma^{2} > 0$
					- definisco $Z_{n} = \frac{\bar{X}_{n} - \mu}{\frac{(\sigma)}{\sqrt{n}}}$ --> stiamo standardizzando! di conseguenza
						- $\mathbb{E}[Z_{n}] = 0$ e $Var(Z_{n}) = 1$
					- allora il teorema ci dice che $$F_{Z_{n}}(x) := \mathbb{P}(Z_{n} \leq x) \to_{n \to +\infty} \Phi(x) \ \ \ \forall x \in \mathbb{R}$$
					- dove $\Phi$ è la funzione di ripartizione della normale standard: _quando $n$ cresce, diventa simile alla funzione di ripartizione della normale standard_
					- $Z_{n} \approx Z$
				- quindi, per questo teorema si ha che $\bar{X}_{n} \approx \frac{\sigma}{\sqrt{n}} z + \mu$, per $n \to +\infty$
				- ma allora $\mathbb{E}[|\bar{X}_{n} - \mu|] \eqsim \frac{\sigma}{\sqrt{n}} \mathbb{E}[|z|] = \frac{\sigma}{\sqrt{n}} \cdots = \cdots \simeq \frac{1}{\sqrt{n}}$

## Domande
- ma perché

## Referenze
