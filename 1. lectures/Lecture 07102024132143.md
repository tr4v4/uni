---
tags:
  - category/lecture
  - status/finished
  - topic/calcolo-numerico
date: 07-10-2024 13:21:43
teacher: elena.loli@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- ripasso introduzione autovalori e autovettori
	- l'obiettivo è di vedere altre fattorizzazioni matriciali
	- se $A$ ha $n$ autovalori distinti allora è diagonalizzabile e si può scrivere come $A = VDV^{-1}$, ossia decomporre $A$ agli autovalori
- ora introduciamo un'altra fattorizzazione utile per risolvere il problema dei minimi quadrati
	- ma prima definizioni/nozioni in più:
		- vettori ortogonali, definizione (già vista)
		- vettori ortonormali, sono ortogonali tra di loro e hanno lunghezza unitaria
		- matrici ortogonali, se le sue colonne sono vettori ortonormali
			- proprietà
				- matrice $A$, se $A$ è ortogonale allora $A^{-1}=A^{T}$
				- $\forall x \in \mathbb{R}^{n}, \ \ \|x\|_{2} = \|Wx\|_{2}$ moltiplicare per una matrice ortogonale mantiene le distanze, o formalmente $W$ è una isometria
	- fattorizzazione in valori singolari (SVD, Singular Value Decomposition)
		- si applica ad una matrice qualsiasi (punti in più rispetto alla fattorizzazione agli autovalori)
		- data una matrice $A_{m \times n}$, la SVD costruisce basi ortogonali di $\mathbb{R}^{m}$ e $\mathbb{R}^{n}$ che permettono di rappresentare $A$ attraverso una matrice diagonale
		- teorema: sia $A$ una matrice reale $m \times n$ di rango $k$, con $k \leq n \leq m$ (caso esempio, nessuno vieta che $k \leq m \leq n$), esistono:
			- $U$ matrice ortogonale $m \times m$
			- $V$ matrice ortogonale $n \times n$
			- $\Sigma$ matrice diagonale $m \times n$
			- tali che
				- $$A = U \Sigma V^{T}$$
				- dove $\Sigma = \begin{bmatrix} \sigma_{1} & \\ & \sigma_{2} \\ \cdots & \cdots & \cdots \\ & & \sigma_{n} \\ & 0 & \end{bmatrix}$, con $\sigma_{k}$ sulla diagonale principale e 0 su tutto il resto
		- proprietà della fattorizzazione:
			- gli elementi $\sigma_{1} \geq \sigma_{2} \geq \cdots \geq \sigma_{n} \geq 0$ sono i valori singolari di $A$
			- il rango $k$ è uguale al numero di valori singolari positivi, gli altri sono nulli, ossia $k = rk(A) \iff \sigma_{1} \geq \cdots \geq \sigma_{k} > 0$ e $\sigma_{k+1} = \cdots = \sigma_{n} = 0$
			- le colonne di $U$ e $V$ sono dette rispettivamente vettori singolari sinistri e destri di $A$, associati ai valori singolari $\sigma_{i}$ e formano una base ortonormale di $\mathbb{R}^{m}$ e $\mathbb{R}^{n}$
			- $A^{T}A = (U \Sigma V^{T})^{T}(U \Sigma V^{T}) = V \Sigma^{T} U^{T} U \Sigma V^{T} = V \Sigma^{T} \Sigma V^{T} = V \begin{bmatrix}\sigma_{1}^{2} & & \\ & \ddots & \\ & & \sigma_{n}^{2}\end{bmatrix} V^{T}$, e sappiamo che $\sigma_{i} = \sqrt{\lambda_{i}(A^{T}A)}$
				- _il risultato è una decomposizione agli autovalori della matrice $A^{T}A$_!
				- quindi esiste una relazione tra gli autovalori di $A^{T}A$ e i valori singolari di $A$
				- formalmente $\sigma_{i}^{2}(A) = \lambda_{i}(A^{T}A) \ \ \ i=1, \cdots, n$, e nota bene la notazione significa "il valore singolare quadrato di $A$" e "l'autovalore di $A^{T}A$"
			- la stessa cosa si può fare per $AA^{T}$, che ricordiamo essere diversa da $A^{T}A$
				- il risultato è che $AA^{T} = U \begin{bmatrix}\sigma_{1}^{2} & & \\ & \ddots & \\ & & \sigma_{n}^{2} \\ & & & 0\end{bmatrix} U^{T}$
				- di nuovo una decomposizione agli autovalori, e vale $\sigma_{i}^{2}(A) = \lambda_{i}(AA^{T}) \ \ \ i = 1, \cdots, n$
			- in particolare:
				- $\sigma_{1} = \sqrt{\lambda_{\max}(A^{T}A)} = \sqrt{\rho(A^{T}A)} = \|A\|_{2}$
				- $\sigma_{k} = \sqrt{\lambda_{\min}(A^{T}A)} \implies \|A^{-1}\|_{2} = \frac{1}{{\sigma_{k}}}$
				- di conseguenza il numero di condizione è facilissimo da calcolare, infatti $K(A) = \|A\| \cdot \|A^{-1}\| = \frac{\sigma_{1}}{\sigma_{k}}$, e sappiamo che se questo rapporto è molto grande allora la matrice è malcondizionata
- problema dei minimi quadrati: $\min_{x \in \mathbb{R}^{n}} {\|Ax - b\|_{2}}^{2}$
	- ripasso del problema: in poche parole ho più equazioni che incognite, generalmente (se il rango di $A$ è uguale ad $n$, ossia il numero di colonne) non esiste una soluzione
	- cerco allora una $x \in \mathbb{R}^{n} : x = \arg_{\min}{{\|Ax-b\|_{2}}^{2}}$, ossia una soluzione che minimizzi la differenza $Ax-b$
		- attenzione: $\min$ è la $y$, mentre $\arg_{\min}$ è $x$, e noi cerchiamo $x$
	- allora chiamiamo $F: \mathbb{R}^{n} \to \mathbb{R}^{+}$ tale che $F(x) = {\|Ax-b\|_{2}}^{2}$
		- è un **problema di ottimizzazione**, per trovare minimi e massimi
		- ma $F$ è particolare, lineare rispetto all'incognita, e ci concentriamo solo sul trovare il minimo
	- anzitutto $Ax - b = r$, chiamato _residuo_; bisogna minimizzare il residuo
	- proposizione: sia $A$ una matrice $m \times n$ con $m > n$ e $rk(A) = k \leq n$, allora il problema dei minimi quadrati ammette sempre almeno una soluzione, e se
		- $k = n \implies$ il problema ammette una e una sola soluzione;
		- $k < n \implies$ il problema ha infinite soluzioni, e queste formano un sottospazio di $\mathbb{R}^{n}$ di dimensione $n-k$.
	- caso rango massimo $k = n$ --> equazioni normali
		- ${\|Ax-b\|_{2}}^{2} = x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b$
		- allora poniamo $F(x) = x^{T}A^{T}Ax - 2x^{T}A^{T}b + b^{T}b$
		- e per minimizzare $F$ bisogna porre la derivata prima a 0; ma abbiamo come incognita un vettore $x \in \mathbb{R}^{n}$, per cui bisogna porre il gradiente a $\underline{0}$
		- ricordiamo che $F: \mathbb{R}^{n} \to \mathbb{R}$, si ha che $\nabla F = \left( \frac{\partial F}{\partial x_{1}}, \cdots, \frac{\partial F}{\partial x_{n}} \right)$
			- inoltre $x^{*} \text{ minimo} \implies \nabla F(x^{*}) = \underline{0}$ ma non viceversa!
		- ora calcoliamo il gradiente di $F$:
			- $\nabla F(x^{T}A^{T}Ax) = 2A^{T}Ax$
			- $\nabla F(2x^{T}A^{T}b) = 2A^{T}b$
			- $\nabla F(b^{T}b) = 0$
		- per cui $\nabla F(x) = 2A^{T}Ax - 2A^{T}b$
		- se imponiamo che il gradiente sia nullo si ottiene il _sistema delle equazioni normali_: $$A^{T}Ax = A^{T}b$$
		- queste due sono chiamate equazioni normali, proprietà:
			- $A^{T}A$ è $n \times n$, ed è _simmetrica semidefinita positiva_; inoltre se $rk(A) = n$ allora è proprio definita positiva
			- $A^{T}b \in \mathbb{R}^{n}$
			- per cui il sistema delle equazioni normali è un sistema lineare quadrato con matrice simmetrica e definita positiva --> il punto di minimo si calcola risolvendo il sistema lineare, magari aiutandomi con la [[Fattorizzazione di Cholesky|fattorizzazione di Cholesky]] (date le proprietà della matrice)
				- con la fattorizzazione ottengo $A^{T}A = LL^{T}$ e si risolvono nell'ordine i due sistemi $Ly = A^{T}b$ e $L^{T}x = y$
			- attenzione: $K(A^{T}A) = K(A)^{2}$, per cui il numero di condizione di $A^{T}A$ è più grande di $K(A)$ perché $K(A) \geq 1$
			- codice d'esempio
	- caso rango $k < n$ (anche se in realtà questo metodo vale anche per $k \leq n$, ma per $k = n$ conviene usare le equazioni normali)
		- ci sono infinite soluzioni, allora ne calcolo una sola $\bar{x}$ di norma minima: $\bar{x} = \min{{\|y\|_{2}}^{2}}$ con $y \in S$ ed $S$ l'insieme delle infinite soluzioni del problema
		- la calcoliamo usando la SDV
		- teorema: riassume la formula ottenuta successivamente
		- passaggi:
			- $A = U \Sigma V^{T}$
			- scrivo ${\|Ax-b\|_{2}}^{2} = {\|U^{T}Ax - U^{T}b\|_{2}}^{2} = {\|U^{T}AVV^{T}x - U^{T}b\|_{2}}^{2} = {\|\Sigma y - g\|_{2}}^{2}$ dove $y = V^{T}x$ e $g=U^{T}b$
				- nota bene: il primo passaggio lo posso fare perché $U$ è ortogonale, allora anche $U^{T}$ è ortogonale e una matrice ortogonale è isometrica
			- detto ciò, ${\|\Sigma y - g\|_{2}}^{2} = \sum\limits_{i=1}^{k} (\sigma_{i}y_{i}-g_{i})^{2} + \sum\limits_{i=k+1}^{n}{g_{i}}^{2}$
			- il secondo termine non serve alla minimizzazione, allora devo minimizzare $\sum\limits_{i=1}^{k} (\sigma_{i}y_{i}-g_{i})^{2}$, rendendo nullo $\sigma_{i}y_{i} - g_{i}$, allora $y_{i} = \frac{g_{i}}{\sigma_{i}} \ \ \ \forall i=1, \cdots, k$
			- e $y_{i}=\frac{g_{i}}{\sigma_{i}} = \frac{U^{T}b}{\sigma_{i}}$, e sapendo che $x^{*} = Vy$, allora la soluzione di minima norma è $$x^{*} = \sum\limits_{i=1}^{k} \frac{u_{i}^{T}b}{\sigma_{i}}v_{i}$$
		- codice d'esempio
	- definizione di pseudoinversa
		- se $A=U \Sigma V^{T}$, decomposta in SVD, allora la pseudoinversa è $$A^{+} = V \Sigma^{+}U^{T}$$
		- dove $\Sigma^{+}$ è la matrice che ha sulla diagonale $\frac{1}{\sigma_{i}}$ (per $i \leq k$)
		- proprietà (poco importanti)
		- funzione principale: scrive la soluzione del problema dei minimi quadrati in modo simile a un sistema lineare, infatti $x^{*} = V \Sigma^{+}U^{T}b \implies x^{*} = A^{+}b$
		- si può definire il condizionamento di $A$ sul problema dei minimi quadrati in termini della pseudoinversa (visto che l'inversa di $A$ non esiste): $$K(A) = \|A\| \cdot \|A^{+}\|$$
			- o il numero di condizionamento spettrale $K(A)_{2} = \|A\|_{2} \cdot \|A^{+}\|_{2} = \frac{\sigma_{1}}{\sigma_{n}}$
	- utilizzi:
		1. approssimazione dati
		2. imaging

## Domande

## Referenze
