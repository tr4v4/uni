---
tags:
  - category/lecture
  - status/finished
  - topic/algebra-e-geometria
date: 16-04-2024 09:16:15
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- recap cambio di base
	- importanza del ricavare $I_{\beta c}$
	- esercizio: determinare se possibile un'applicazione lineare $f: \mathbb{R}^{3} \to \mathbb{R}^{3}$ tale che $\ker(f) = \langle e_{1}+e_{2}+2e_{3} \rangle = \langle (1, 1, 2) \rangle$ e $\Im(f) = \langle e_{1}-e_{2}, e_{2}+7e_{3} \rangle = \langle (1, -1, 0), (0, 1, 7) \rangle$ e scrivere la matrice associata ad $f$ rispetto alla base canonica
		- quindi prima controlliamo la compatibilità con il teorema della dimensione: $3 = \dim(\ker(f)) + \dim(\Im(f)) = 1 + 2$ quindi sì
		- per costruire $f$ procediamo in questo modo: so che $f(1, 1, 2) = \underline{0}$, per cui scegliamo una base che contenga questo vettore $\beta = \{(1, 1, 2), (0, 1, 0), (0, 0, 1)\} = \{v_{1}, v_{2}, v_{3}\}$
		- poniamo $f(1, 1, 2) = (0, 0, 0)$, $f(0, 1, 0) = (1, -1, 0)$ e $f(0, 0, 1) = (0, 1, 7)$, questo perché $\Im(f) = \langle f(v_{1}), f(v_{2}), f(v_{3}) \rangle$, e quindi $f$ è univocamente determinata da quelle immagini e $\Im(f)$ è generata da quei 3 vettori
		- per cui abbiamo trovato una $f$, che sappiamo esistere
		- <u>osservazione</u>: sappiamo che $\langle (1, 1, 2) \rangle \leq \ker(f)$, ma non mi dice che il nucleo è proprio uguale, MA per il teorema della dimensione si ha che $\dim(\ker(f)) = 1$ perciò $\langle (1, 1, 2) \rangle = \ker(f)$
		- vogliamo ora scrivere la matrice associata a $f$
			- $A_{\beta c} = ((f(v_{1}))_{c}, (f(v_{2}))_{c}, (f(v_{3}))_{c}) = \begin{pmatrix} 0 & 1 & 0 \\ 0 & -1 & 1 \\ 0 & 0 & 7 \end{pmatrix}$
			- ora noi vogliamo $A_{cc}$, quindi solito schemino
			- viene fuori $$A_{cc} = A_{\beta c} \cdot I_{c \beta} = A_{\beta c} \cdot I_{\beta c}^{-1}$$
			- ci troviamo $I_{\beta c}$ mettendo in colonna i vettori della base $\beta$, per cui $$I_{\beta c} = \begin{pmatrix} 1 & 0 & 0 \\ 1 & 1 & 0 \\ 2 & 0 & 1 \end{pmatrix}$$
			- e quindi calcoliamo la sua inversa $I_{\beta c}^{-1}$ usando Gauss, ci viene $$I_{\beta c}^{-1} = \begin{pmatrix} 1 & 0 & 0 \\ -1 & 1 & 0 \\ -2 & 0 & 1 \end{pmatrix}$$
		- altra idea: troviamo $f$ usando la base canonica
			- sappiamo $f(e_{1} + e_{2} + 2e_{3}) = \underline{0}$, per cui per la sua linearità $f(e_{1}) + f(e_{2}) + 2f(e_{3}) = \underline{0} \implies f(e_{1}) = -f(e_{2}) - 2f(e_{3})$
			- quindi mandiamo $f(e_{2}), f(e_{3})$ dove vogliamo, e poi obblighiamo $f(e_{1})$ a rispettare l'equazione
			- possiamo fare $f(e_{2}) = (1, -1, 0)$ e $f(e_{3}) = (0, 1, 7)$ (così rispettiamo l'immagine), e poi $f(e_{1}) = -(1, -1, 0) - 2(0, 1, 7) = (-1, -1, -14)$
			- e ora ci costruiamo la matrice, quindi metodo molto veloce
- autovalori, autovettori e diagonalizzabilità di matrici
	- definizione (si può dividere per autovettore e autovalore): sia $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ lineare, allora un vettore $v \in \mathbb{R}^{n}$ si dice autovettore di autovalore $\lambda$ se $v \neq \underline{0}$ (condizione tecnica) e $f(v) = \lambda v$ con $\lambda \in \mathbb{R}$
		- un'applicazione lineare deforma lo spazio dei vettori su cui viene applicata, ebbene ci sono alcuni vettori che rimangono nella stessa retta (in generale sullo stesso span), mantenendo la loro direzione --> vengono mappati in un multiplo di loro stessi
	- quindi: come troviamo gli autovettori? perché sono importanti?
	- sono importanti quando faccio il cambio di base
		- _voglio cambiare la base e fare in modo che la matrice associata sia diagonale, per questioni di comodità_
		- definizione: una matrice quadrata si dice diagonale se ha tutti 0 fuori dalla diagonale principale
			- esempio: la matrice nulla è diagonale, così come la matrice identità
		- definizione: $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ lineare si dice diagonalizzabile se esiste una base $\beta$ di $\mathbb{R}^{n}$ t.c. $M_{\beta}^{\beta} (f)$ (la matrice associata ad $f$ rispetto alla base $\beta$ in dominio e codominio) sia diagonale
		- esempio: $f: \mathbb{R}^{2} \to \mathbb{R}^{2}$ t.c. $f(e_{1}) = e_{2}, f(e_{2}) = e_{1}$, per cui $f$ flippa lo spazio $\mathbb{R}^{2}$, simmetria rispetto alla retta $y=x$, infatti $(x_{1}, x_{2}) \to (x_{2}, x_{1})$
			- per esempio: $(2, 1) \to (1, 2)$
			- ci sono autovettori --> tutti quelli sulla retta $y=x$ e su $y=-x$, per esempio
				- $v_{1} = (1, 1) \to (1, 1)$, con autovalore $1$;
				- $v_{2} = (1, -1) \to (-1, 1)$, con autovalore $-1$;
			- osserviamo che $\beta = \{v_{1}, v_{2}\}$ è base di $\mathbb{R}^{2}$
			- ora troviamo $A_{\beta\beta}$
				- $A_{\beta\beta} = ((f(v_{1}))_{\beta}, (f(v_{2}))_{\beta}) = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$
				- l'idea è che **se prendo una base di autovettori la matrice viene _sempre_ diagonale**
		- altro esempio: $f: \mathbb{R}^{2} \to \mathbb{R}^{2}$ t.c. $f(e_{1}) = e_{2}, f(e_{2}) = -e_{1}$, ovvero la rotazione di 90° gradi in senso antiorario ($\frac{\pi}{2}$) --> in questo caso non riusciamo a trovare degli autovettori, perché lo spazio ruota tutto --> nessun vettore conserva la sua direzione
			- vogliamo far vedere che se non ci sono autovettori non possiamo diagonalizzare la matrice associata ad $f$!
			- dimostrazione per assurdo: supponiamo di avere una base $\beta = \{v_{1}, v_{2}\}$ t.c. $A_{\beta\beta} = \begin{pmatrix} \lambda_{1} & 0 \\ 0 & \lambda_{2} \end{pmatrix}$ (è diagonale); sappiamo $A_{\beta\beta} = ((f(v_{1}))_{\beta}, (f(v_{2}))_{\beta})$, quindi $(f(v_{1}))_{\beta} = (\lambda_{1}, 0)$ e $(f(v_{2}))_{\beta} = (0, \lambda_{2})$, perciò $f(v_{1}) = \lambda_{1}v_{1}$ perciò $v_{1}$ è un autovettore, ma questo è un assurdo perché per ipotesi non ci sono autovettori
		- morale: per cui c'è un legame molto stretto tra autovettori e diagonalizzabilità
	- teorema che riassume proprio tutto ciò: sia $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ lineare, allora $f$ è diagonalizzabile $\iff$ esiste una base di $\mathbb{R}^{n}$ costituita da autovettori di $f$
		- per cui la base "bella" che diagonalizza la matrice dev'essere proprio una base di autovettori
		- dimostrazione:
			- $\impliedby$: supponiamo che $\beta = \{v_{1}, \cdots, v_{n}\}$ sia una base di autovettori e mostriamo che $A_{\beta\beta}$ è diagonale
				- allora $f(v_{1}) = \lambda_{1}v_{1}, \cdots, f(v_{n}) = \lambda_{n}v_{n}$
				- allora $A_{\beta\beta} = ((f(v_{1}))_{\beta}, \cdots, (f(v_{n}))_{\beta}) = ((\lambda_{1}, 0, \cdots, 0), \cdots, (0, \cdots, 0, \lambda_{n}))$, infatti $f(v_{i})_{\beta} = (0, \cdots, \lambda_{i}, \cdots, 0)$
				- e $A_{\beta\beta}$ viene allora proprio diagonale
			- $\implies$: supponiamo $A_{\beta\beta}$ è diagonale, e $\beta = \{v_{1}, \cdots, v_{n}\}$ una base t.c. $A_{\beta\beta}$ è diagonale e mostriamo che $\beta$ è una base di autovettori
				- allora $A_{\beta\beta} = ((\lambda_{1}, 0, \cdots, 0), \cdots, (0, \cdots, 0, \lambda_{n})) = ((f(v_{1}))_{\beta}, \cdots, (f(v_{n}))_{\beta})$
				- quindi $f(v_{1})_{\beta} = (\lambda_{1}, 0, \cdots, 0) \implies f(v_{1}) = \lambda_{1}v_{1}$, per cui $v_{1}$ autovettore
				- e così per tutti gli $n$ vettori di $\beta$
		- nota bene: gli elementi sulla diagonale sono allora gli autovalori!

## Domande
- counter lavagne = 35 (NUOVO RECORD)

## Referenze
