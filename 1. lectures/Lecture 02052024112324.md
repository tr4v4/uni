---
tags:
  - category/lecture
  - status/pending
  - topic/algebra-e-geometria
date: 02-05-2024 11:23:24
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- sottospazio ortogonale, ripasso
	- $W^{\bot} = \{v \in V | <v, w> = 0 \ \ \forall w \in W\}$
	- proposizione: $\dim(W^{\bot}) = n - \dim(W)$
- **proiezione ortogonale di un vettore su un sottospazio**
	- sia $\beta_{1} = \{w_{1}, \cdots, w_{k}\}$ base ortonormale di $W$ (riusciamo sempre a trovare una base ortonormale, con l'algoritmo di Gram-Schmidt) e $\beta_{2} = \{z_{1}, \cdots, z_{n-k}\}$ base ortonormale di $W^{\bot}$;
	- consideriamo $\beta = \beta_{1} \cup \beta_{2}$ allora $\beta$ contiene $k + (n-k) = n$ vettori tutti ortogonali tra loro e linearmente indipendenti, per cui $\beta$ è una base di $\mathbb{R}^{n}$, anzi base ortonormale di $\mathbb{R}^{n}$;
	- allora sia $v \in \mathbb{R}^{n}$ per cui $v = \lambda_{1}w_{1} + \cdots \lambda_{k}w_{k} + \gamma_{1}z_{1} + \cdots + \gamma_{n-k}z_{n-k}$, e lo scriviamo come $v = v_{1} + v_{2}$ dove $v_{1} \in W$ e $v_{2} \in W^{\bot}$
	- quindi ogni $v \in V$ si scrive come $v = v_{1} + v_{2}$ con $v_{1} \in W$ e $v_{2} \in W^{\bot}$, allora _diciamo che $v_{1}$ è la proiezione ortogonale di $v$ su $W$_ --> stiamo facendo la proiezione ortogonale di un vettore su un sottospazio
	- nota: $v_{1} = \lambda_{1}v_{1} + \cdots + \lambda_{k}w_{k}$ e $\lambda_{i} = <v, w_{i}>$ perché $\beta$ base ortonormale (questo da una proposizione della lezione precedente)
		- allora per trovare $v_{1}$ basta conoscere una base ortonormale di $W$
		- $v_{1} = <v, w_{1}>w_{1} + \cdots + <v, w_{k}>w_{k}$ con $\beta_{1} = \{w_{1}, \cdots, w_{k}\}$
	- osservazione: non è necessario avere una base ortonormale di $W$, basta una base ortogonale, infatti
		- sia $\{u_{1}, \cdots, u_{k}\}$ base ortogonale di $W$, allora $\{f_{1}, \cdots, f_{k}\}$ è base ortonormale di $W$ dove $f_{i} = \frac{u_{i}}{||u_{i}||}$
		- allora abbiamo $v_{1} = <v, f_{1}>f_{1} + \cdots + <v, f_{k}>f_{k} = <v, \frac{1}{||u_{1}||}u_{1}> \frac{1}{||u_{1}||}u_{1} + \cdots + <v, \frac{1}{||u_{k}||}u_{k}> \frac{1}{||u_{k}||}u_{k} = \frac{1}{||u_{1}||}<v, u_{1}> \frac{1}{||u_{1}||}u_{1} + \cdots + \frac{1}{||u_{k}||}<v, u_{k}> \frac{1}{||u_{k}||}u_{k}$
		- e allora $v_{1} = \frac{<v, u_{1}>}{<u_{1}, u_{1}>}u_{1} + \cdots + \frac{<v, u_{k}>}{<u_{k}, u_{k}>}u_{k}$
		- che è la generalizzazione della formula di prima ma con lunghezze non $1$ dei vettori $u_{1}, \cdots, u_{k}$
- nuovo argomento (simile): **matrici ortogonali**, **matrici simmetriche** e **teorema spettrale**
	- **matrice ortogonale**
		- definizione: un'applicazione lineare $f$ che va da $\mathbb{R}^{n}$ a $\mathbb{R}^{n}$ si dice ortogonale se $$<f(u), f(v)> = <u, v> \ \ \ \forall u, v \in \mathbb{R}^{n}$$, ovvero se conserva il prodotto scalare
		- in particolare $$||f(v)|| = \sqrt{<f(v), f(v)>} = \sqrt{<v, v>} = ||v||$$
		- quindi _$f$ conserva le lunghezze e conserva la perpendicolarità_, infatti $f(u) \bot f(v) \iff <f(u), f(v)> = 0 \iff <u, v> = 0 \iff u \bot v$
		- dunque $f$ conserva gli angoli
		- molto importante per gli ingegneri --> bisogna poter ruotare o muovere gli oggetti nello spazio senza deformarli, e lo si fa con applicazioni lineari ortogonali
		- definizione: una matrice $A \in M_{n}(\mathbb{R})$ si dice ortogonale se $A \cdot A^{T} = I$, ovvero se $A^{T} = A^{-1}$
		- proposizione: sia $A \in M_{n}(\mathbb{R})$ allora sono equivalenti:
			1. $<Ax, Ay> = <x, y> \ \ \forall x, y \in \mathbb{R}^{n}$
			2. $<Ax, Ax> = <x, x> \ \ \ \forall x, y \in \mathbb{R}^{n}$
			3. $A \cdot A^{T} = I$
			4. le colonne (rispettivamente le righe) di $A$ sono una base ortonormale di $\mathbb{R}^{n}$
			- dimostrazione ($3 \equiv 4$)
				- $AA^{T} = I \iff A^{T}A = I$, e sia $A = (c_{1}, \cdots, c_{n})$, allora $A^{T}$ ha le colonne $c_{1}, \cdots, c_{n}$ come righe
				- quindi l'elemento di posto $ij$ di $B = A^{T}A$ $b_{ij} = \delta_{ij}$, ovvero $b_{ij} = \begin{cases}0 & i \neq j \\ 1 & i = j\end{cases}$, quindi gli $1$ sulla diagonale
				- ma $b_{ij} = <c_{i}, c_{j}>$, quindi $A^{T}A = I \iff$ le colonne $c_{i}$ sono ortonormali
				- infatti, esempio
					- $b_{11} = 1 \iff <c_{1}, c_{1}> = 1$
					- $b_{12} = 0 \iff <c_{1}, c_{2}> = 0$
					- $b_{13} = 1 \iff <c_{1}, c_{3}> = 0$
	- **teorema spettrale**
		- gli spettri sono gli autovalori!
		- definizione: un endomorfismo si dice simmetrico se $<f(u), v> = <u, f(v)> \ \ \forall u, v \in \mathbb{R}^{n}$
		- definizione: matrice simmetrica
		- proposizione: sia $f$ endomorfismo e sia $A$ la matrice associata ad $f$, allora $f$ è simmetrica $\iff$ $A$ è simmetrica
		- teorema spettrale: sia $f: \mathbb{R}^{n} \to \mathbb{R}^{n}$ endomorfismo simmetrico, allora esiste una base di $\mathbb{R}^{n}$ costituita da autovettori di $f$
			- teorema potentissimo
			- in particolare per corollario si ha:
				1. un endomorfismo simmetrico è sempre diagonalizzabile
				2. se $\lambda_{1}, \lambda_{2}$ sono autovalori distinti, allora $V_{\lambda_{1}} \bot V_{\lambda_{2}}$, cioè autospazi relativi ad autovalori distinti sono ortogonali
				3. se $M_{c}^{c}(f) = A$ allora esistono una matrice ortogonale $P$ e una matrice diagonale $D$ tale che $P^{-1}AP = P^{T}AP = D$, cioè $A$ è ortogonalmente simile a una matrice diagonale
		- esempio: sia $A = \begin{pmatrix}1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1\end{pmatrix}$, determinare una matrice ortogonale $P$ tale che $P^{T}AP = D$ con $D$ diagonale
			- $P$ ortogonale $\implies P^{-1}=P^{T}$
			- $A$ simmetrica $\implies$ $A$ diagonalizzabile, per il teorema spettrale
			- ci procuriamo una base ortonormale costituita da autovettori
				- quindi prima ci troviamo una base dei singoli autospazi
				- poi ortonormalizziamo ogni base con Gram-Schmidt
				- e infine uniamo le basi e otteniamo una base ortonormale di autovettori
			- $I_{\beta c}^{-1}AI_{\beta c} = D$ ma $I_{\beta c}^{-1} = I_{\beta c}^{T}$ perché la base è ortonormale
			- procediamo come di consueto
				- $p_{A}(x) = \det(A - xI) = -x^{3}+3x^{2} = -x^{2}(x-3)$
				- nota bene: ci aspettiamo l'autovalore 0 con molteplicità algebrica e anche geometrica 2, perché la dimensione del kernel (autospazio associato all'autovalore 0) è 2, essendo la dimensione delle colonne 1
				- infatti gli autovalori sono
					- $\lambda_{1} = 0, m_{a}(0) = 2$
					- $\lambda_{2} = 3, m_{a}(3) = 1$
				- troviamo gli autospazi, facendo i conti viene
					- $V_{0} = \langle (-1, 1, 0), (-1, 0, 1) \rangle$
					- $V_{3} = \langle (1, 1, 1) \rangle$
			- abbiamo trovato allora che $\bar{\beta} = \{(-1, 1, 0), (-1, 0, 1), (1, 1, 1)\}$ è una base di autovettori
			- ma noi vogliamo una base ortonormale di autovettori
				- se li prendo in autospazi diversi sono perpendicolari tra di loro, ma quelli nello stesso autospazio non è detto che siano perpendicolari
				- per cui dobbiamo rendere perpendicolari $(-1, 1, 0), (-1, 0, 1)$ tra di loro, perché facendo il prodotto scalare non viene 0
				- usiamo Gram-Schmidt:
					- $u_{1} = v_{1}$
					- $u_{2} = v_{2} - proj_{u_{1}}(v_{2}) = (- \frac{1}{2}, - \frac{1}{2}, 1)$
			- allora troviamo che $\bar{\beta} = \{(-1, 1, 0), (- \frac{1}{2}, - \frac{1}{2}, 1), (1, 1, 1)\}$ è una base ortogonale di autovettori, ma noi la vogliamo ortonormale! --> normalizziamo tutto
			- alla fine dei conti otteniamo $\beta$
			- da $\beta$ possiamo trovare $P = I_{\beta c}$ e $D$ è la matrice diagonale con gli autovalori sulla diagonale
		- nota bene: trovare una base ortogonale di $W$ $\neq$ trovare una base di $W^{\bot}$

## Domande

## Referenze
