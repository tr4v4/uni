---
tags:
  - category/lecture
  - status/finished
  - topic/calcolo-numerico
date: 30-09-2024 13:19:14
teacher: elena.loli@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- ripasso fattorizzazione LU
	- la matrice delle permutazioni $P$ serve a:
		1. garantire il funzionamento dell'algoritmo su matrici che non hanno tutti i minori principali diversi da 0
		2. limitare l'errore algoritmico per elementi sulla diagonale $a_{ii}$ molto piccoli
- caso particolare
	- $A$ matrice simmetrica definita positiva
		- ossia $\forall x \in \mathbb{R}^{n}$ vale $x^{T}Ax > 0$ (o se semi-definita positiva $x^{T}Ax \geq 0$)
	- vale però che se $A$ (simmetrica) è definita positiva allora gli autovalori di $A$ sono tutti positivi (o nulli se semi-definita positiva)
	- avviene che in questo caso la fattorizzazione LU prende un altro nome: **fattorizzazione di Cholesky**, per cui $A = L \cdot L^{T}$ dove $L$ è una matrice triangolare inferiore ($L^{T}$ triangolare superiore)
		- la complessità computazionale risulta essere $O\left(\frac{n^{3}}{6}\right)$
		- funziona allo stesso modo, ma non si deve trovare $U$, perché è la trasposta di $L$
	- in _scipy_ (libreria Python) la funzione è `scipy.linalg.cholesky(A)`
- analisi dell'**errore inerente**
	- bisogna "introdurre" la _norma_, sia vettoriale che matriciale
	- serve per stabilire un numero che misuri l'errore, la distanza della soluzione ottenuta da quella effettiva, per vettori e matrici (altrimenti per scalari basterebbe il valore assoluto)
	- norma vettoriale
		- definizione: $|| \cdot || : \mathbb{R}^{n} \to \mathbb{R}$ tale che
			1. $||x|| \geq 0 \ \ \ \forall x \in \mathbb{R}^{n}$, e $||x|| = 0 \iff x = \underline{0}$
			2. $\alpha \in \mathbb{R}, \| \alpha x \| = |\alpha| \|x\|$
			3. $\forall x, y \in \mathbb{R}^{n}, \|x+y\| \leq \|x\|+\|y\|$ (disuguaglianza triangolare)
		- la più utilizzata è la [[Norma euclidea|norma euclidea]] (anche chiamata norma 2)
			- è definita come $x = (x_{1}, \cdots, x_{n}), \|x\| = \sqrt{{x_{1}}^{2} + \cdots + {x_{n}}^{2}}$
		- ma ce ne sono anche altre
			- $\|x\|_{1} = |x_{1}| + \cdots + |x_{n}|$
	- norma matriciale
		- che senso ha misurare una matrice? di un vettore si capisce cosa sia, delle matrici è per capire la distanza tra due matrici
		- formalmente è un'estensione della norma vettoriale
		- definizione: $\| \cdot \|: \mathbb{R}^{n} \times \mathbb{R}^{m} \to \mathbb{R}$ tale che
			1. $\|A\| \geq 0 \ \ \ \forall A \in \mathbb{R}^{n \times m}, \|A\| = 0 \iff A = \underline{\underline{0}}$
			2. $\alpha \in \mathbb{R}, \|\alpha A\| = |\alpha| \|A\|$
			3. $\forall A, B \in \mathbb{R}^{n \times m}, \|A+B\| \leq \|A\| + \|B\|$
		- tipologie:
			- _norma di frobenius_
				- $\|A\|_{F} = \sqrt{\sum\limits_{i=1}^{m}{\sum\limits_{j=1}^{n}{{a_{ij}}^{2}}}}$, quindi è l'equivalente matriciale della norma euclidea
				- nota bene: $\|I_{3}\|_{F} = \sqrt{3}$, o in generale $\|I_{n}\|_{F} = \sqrt{n}$
			- _norma 2_
				- $\|A\|_{2} = \sqrt{p(A^{T}A)}$, dove $p(M)$ è il _raggio spettrale_, ossia il _massimo autovalore in modulo_
			- _norma 1_
				- $\|A\|_{1} = \max_{1 \leq i \leq m} \sum\limits_{j=1}^{n}{|a_{ij}|}$
	- ora, supponiamo che la soluzione esatta di $Ax = b$ sia $x = (1, 1, 1)$, e quella calcolata sia $\bar{x} = (1.01, 0.98, 1)$; qual è l'errore assoluto e l'errore relativo commesso?
		- si ha che $E_{A} = \|x - \bar{x}\| = \|(-0.01, 0.02, 0)\|_{2} = \sqrt{(-0.01)^{2} + (0.02)^{2} + 0^{2}}$
		- e invece $E_{R} = \frac{\|x - \bar{x}\|}{\|x\|} = \frac{E_{A}}{\|x\|_{2}} = \frac{E_{A}}{\sqrt{3}}$
		- è indifferente usare una norma o un'altra
	- funzioni _numpy_
		- `numpy.linalg.norm()` calcola la norma di un oggetto (vettore o matrice)
		- posso specificare il tipo di norma
	- arriviamo all'**errore inerente**
		- finora abbiamo visto l'_errore algoritmico_ (causato dalla finitezza dei numeri) e l'_errore di troncamento_
		- l'errore inerente è dovuto alla rappresentazione con i numeri finiti, ossia all'errore di rappresentazione
			- quindi suppongo che ci sia un errore nella rappresentazione dei dati, ma le operazioni vengono fatte in aritmetica reale --> non ci sono errori nelle operazioni
			- l'errore del risultato finale legato solo alla rappresentazione dei dati è chiamato errore inerente
		- c'è sempre questo errore (mentre, per esempio, quello di troncamento no)
		- ![[Drawing 2024-09-30 14.46.11.excalidraw|750]]
		- il risultato ci dice che $$\frac{\|\Delta x\|}{\|x\|} \leq k(A)\left( \frac{\|\Delta A\|}{\|A\|} + \frac{\|\Delta b\|}{\|b\|} \right)$$
			- $k(A)$ è il **numero di condizione** della matrice, definito come $\|A\| \cdot \|A^{-1}\|$, sempre $\geq 1$
			- quindi, anche se ho un errore di rappresentazione molto basso, a seconda di $k(A)$ posso ottenere un errore inerente gigantesco!
			- se $k(A)$ è molto grande non posso farci niente: non dipende dall'algoritmo, è necessario "spostare" il problema cambiando matrice!
			- la formula ci dice che l'errore sul risultato è $\leq$ del numero di condizione della matrice per l'errore sui dati
		- esempi incredibili in Python
			- ci creeremo un sacco di _problemi test_: a partire dalla soluzione esatta ci si crea i dati; in particolare, nel caso delle matrici, avendo $A$ e $x$ (soluzione esatta), si crea $b$ facendo $b = Ax$
				- quindi si risolve $Ax = b$ e si trova $\bar{x}$, e successivamente si confronta $\bar{x}$ con $x$
				- questo permette di valutare quanto è buono l'algoritmo
			- all'aumentare della dimensione della matrice $A$ aumenta anche $k(A)$
		- il senso è:
			- ho un errore sui dati (relative backward error) e ho un errore sui risultati (relative error) --> questi sono legati da $k(A)$
		- se $k(A) >> 1$ la matrice $A$ si dice _mal condizionata_, e il sistema $Ax = b$ si dice _mal posto_
			- <u>nota bene</u>: "mal posto" è un aggettivo del problema, ossia del sistema, e non dell'algoritmo
- **autovalori e autovettori**
	- l'obiettivo è risolvere un sistema lineare $Ax = b$ dove $A$ è una matrice $m \times n$ con $m > n$
		- ho più equazioni che incognite --> in generale sappiamo che non ha soluzione
	- il problema viene spostato nella domanda: $$\min_{x \in \mathbb{R}^{n}} {\|Ax - b\|_{2}}^{2}$$
		- non potrà mai essere 0 $Ax-b$ (perché $m > n$), vogliamo allora trovare la $x$ che minimizza questa differenza
		- problema estremamente comune, anche nel machine learning!
	- ma prima dobbiamo "introdurre" autovalori e autovettori
		- autovalori: sappiamo già
			- $A$ ha al più $n$ autovalori distinti
			- $\forall$ autovalore esistono infiniti autovettori
			- supponiamo che $Av_{i} = \lambda_{i}v_{i}$ con $i = 1, \cdots, n$, in notazione si scrive $A[v_{1}, \cdots, v_{n}] = [v_{1}, \cdots, v_{n}] \begin{pmatrix} \lambda_{1} & & \\ & \ddots & \\ & & \lambda_{n} \end{pmatrix}$
			- si scrive $AV = VD$, e si chiama decomposizione agli autovalori di $A$
			- si ha quindi che $A = VDV^{-1}$
			- se $A$ ha decomposizione agli autovalori allora è diagonalizzabile
	- in Python, funzioni _numpy_
		- `np.linalg.eig(A)` calcola autovalori e autovettori

## Domande

## Referenze
