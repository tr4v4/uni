---
tags:
  - category/lecture
  - status/finished
  - topic/algoritmi-e-strutture-dati
date: 21-02-2024 11:16:57
teacher: pietro.dilena@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- [[Notazione asintotica|notazione asintotica]]
	- O-grande (aggiungere immagini)
		- diciamo che f è un O-grande di g se g rappresenta un limite asintotico superiore per f, ovvero se f oltre un certo valore non supererà mai il rate di crescita di g
	- Omega-grande
		- il simmetrico di O-grande, ovvero
			- $$\Omega(g(n)) = \{f(n) | \exists c > 0, n_{0} \geq 0 : \forall n \geq n_{0}, f(n) \geq cg(n)\}$$
			- f è Omega-grande di g se da un certo punto in poi la funzione di costo f non potrà mai essere inferiore a g
			- esempio $n^{3}+ 2n^{2} = \Omega(n^{2})$, ovvero $n^{3} + 2n^{2}$ crescerà più velocemente di $n^{2}$
	- Theta
		- diciamo che $f(n) = \Theta(g(n))$ se non si sovrastano, ovvero se l'ordine di crescita di $f$ è uguale a quello di $g$
			- teorema: $f = \Theta(g) \iff f = O(g) \land f = \Omega(g)$
	- utilizzo delle notazioni per classificare algoritmo
		- supponiamo di avere una funzione di costo di un algoritmo pari a $f(n) = n^{3} + 2n^{2}$
		- sappiamo per $\Theta$ che $f$ ha lo stesso growing-rate di $n^{3}$ --> possiamo quindi considerare il costo compiutazione di $f$ come in proporzione cubica rispetto alla dimensione dell'input $n$
			- è semplicemente una notazione comoda
		- utilizzeremo tecniche che ci permettono immediatamente di classificare un algoritmo
	- O-piccolo
		- si differenzia dall'O-grande per avere vincoli più stretti: $f$ cresce SEMPRE meno velocemente di $g$, e il rate di crescita non può essere uguale
		- $$o(g(n)) = \{f(n) | \forall c > 0, n_{0} \geq 0 : \forall n \geq n_{0}, f(n) < cg(n)\}$$
		- se f è O-piccolo di g allora è anche O-grande di g
	- Omega-piccolo
		- stessa relazione tra O-grande e O-piccolo ma per Omega
		- se f è Omega-piccolo di g allora f è anche Omega-grande di g
	- nota bene: in analisi ci si concentra su funzioni generali e per valori specifici (per esempio per l'o-piccolo quando ci si avvicina a 0); in algoritmi intendiamo solo funzioni positive e per valori che tendono a infinito (asintotici)
	- notazione asintotica e limiti
		- $\lim_{n \to +\infty}{\frac{f(n)}{g(n)}} = 0 \implies f(n) = o(g(n)) \implies f(n) = O(g(n))$
		- $\lim_{n \to +\infty}{\frac{f(n)}{g(n)}} = \infty \implies f(n) = \omega(g(n)) \implies f(n) = \Omega(g(n))$
		- $\lim_{n \to +\infty}{\frac{f(n)}{g(n)}} = k > 0 \implies f(n) = \Theta(g(n))$
	- interpretazione intuitiva
		- ![[relazioni-notazione-asintotica.png]]
	- <u>attenzione</u>: ci sono funzioni cui rate di crescita non sono confrontabili
		- esempio: $f(n) = n$ e $g(n) = n^{\sin(n)+1}$
			- $f(n) \neq O(g(n))$
			- $f(n) \neq \Omega(g(n))$
		- <u>nota</u>: data una funzione positiva posso sempre scrivere un algoritmo che ha quella funzione di costo
	- proprietà (da dimostrare)
		- $\forall a > 1, \forall b > 1, \log_{a}{n} = \Theta(\log_{b}{n})$
		- $\forall a > 0, \forall b > 0, \log^{a}{n} = O(n^{b})$
		- $\log{n!} = \Theta(n\log{n})$
		- $1 < a < b, a^{n} = O(b^{n})$
		- $a > 0, n^{\log{a}} = \Theta(a^{\log{n}})$
	- altre proprietà
		- **transitività**: $f(n) = O(g(n)) \land g(n) = O(h(n)) \implies f(n) = O(h(n))$
		- **riflessività**: $f(n) = O(f(n)) \land f(n) = \Omega(f(n)) \land f(n) = \Theta(f(n))$
		- **simmetria**: $g(n) = \Theta(f(n)) \iff f(n) = \Theta(g(n))$
		- **simmetria trasposta**:
			- $f(n) = O(g(n)) \iff g(n) = \Omega(f(n))$
			- $f(n) = o(g(n)) \iff g(n) = \omega(f(n))$
		- **somma** (vale anche per altre relazioni asintotiche): $f_{1}(n) = O(g_{1}(n)) \land f_{2}(n) = O(g_{2}(n)) \implies f_{1}(n) + f_{2}(n) = O(g_{1}(n)) + O(g_{2}(n)) = O(g_{1}(n) + g_{2}(n))$
		- **prodotto** (vale anche per altre relazioni asintotiche): $f_{1}(n) = O(g_{1}(n)) \land f_{2}(n) = O(g_{2}(n)) \implies f_{1}(n) \cdot f_{2}(n) = O(g_{1}(n)) \cdot O(g_{2}(n)) = O(g_{1}(n) \cdot g_{2}(n))$
		- **moltiplicazione per una costante** (vale anche per altre relazioni asintotiche): $f(n) = O(g(n)) \land a > 0 \implies a \cdot f(n) = O(g(n))$
			- + motivo per cui non si può aggiungere "somma per una costante" (ci sono funzioni di costo, non utilizzate, che decrescono, come $\frac{1}{x}$)
	- tabella degli ordini di crescita
		- ![[ordini-di-crescita.png]]
		- ![[confronto-ordini-di-crescita.png]]
- complessità compiutazionale
	- definizione per un algoritmo: un algoritmo $A$ ha complessità compiutazionale $O(f(n))$ rispetto ad una certa risorsa di calcolo se la quantità di risorse necessaria per eseguire $A$ su un qualsiasi input di dimensione $n$ è $O(f(n))$
	- definizione per un problema: un problema $P$ ha complessità compiutazionale $O(f(n))$ rispetto ad una certa risorsa di calcolo se esiste un algoritmo che risolve $P$ con costo compiutazionale $O(f(n))$ rispetto a tale risorsa di calcolo
	- nell'analisi di un algoritmo dobbiamo assegnare una complessità (possibilmente in $\Theta$, altrimenti con $\Omega$ e $O$) su 3 casi:
		- _caso ottimo_: esempio ricerca sequenziale quando l'elemento cercato è il primo
		- _caso pessimo_: esempio ricerca sequenziale quando l'elemento cercato è l'ultimo/non compare
		- _caso medio_: esempio costo medio di una ricerca sequenziale in una lista[^1]
		- solitamente si esclude sia $\Theta$ che $\Omega$, e ci si concentra sul definire $O$, ovvero qual è al peggio il comportamento dell'algoritmo
			- ciononostante non sia la notazione più precisa, e spesso rappresenti solo un _upper-bound_
		- esempio proprio con la ricerca sequenziale
			- caso ottimo: $O(1)$, che sarebbe $\Theta(1)$ ma non si utilizza mai
			- caso pessimo: $\Theta(n)$
			- caso medio: $O(n)$

## Domande
- siamo sicuri che $\forall a > 0, \forall b > 0, \log^{a}{n} = O(n^{b})$? Perché il tutto sembra dipendere strettamente dai valori di $a$ e $b$;

## Referenze
[^1]: $\frac{(n+1)}{2}?$