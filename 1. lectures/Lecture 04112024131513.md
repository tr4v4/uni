---
tags:
  - category/lecture
  - status/finished
  - topic/calcolo-numerico
date: 04-11-2024 13:15:13
teacher: elena.loli@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- **problemi inversi**
	- ultimo argomento del corso
		- argomenti svolti: numeri finiti, zeri di funzione, risoluzione tramite fattorizzazione (LU o Cholesky) di sistemi lineari quadrati, fattorizzazione SVD, problema dei minimi quadrati, approssimazione di dati (con minimi quadrati e interpolazione)
	- i problemi inversi sono un modello matematico
	- si considerano dei fenomeni fisici in cui descrivo un effetto, a partire da una causa, utilizzando un modello
		- _causa -> modello -> effetto_
		- matematicamente _x -> A -> y_, cioè $Ax = y$ se consideriamo un fenomeno lineare
		- questo tipo di problema si chiama **diretto**: input -> output
	- il problema **inverso**, allora consiste nel partire dall'output e trovare input: input -> output
		- nel caso banale del sistema, io conosco il modello $A$ e l'output $y$ e voglio trovare la causa $x$: voglio risolvere il sistema lineare
	- applicazioni dei problemi inversi: sono un ponte che unisce i problemi matematici ai dati, perché a partire dai dati osservati voglio determinare i parametri del modello, ossia la causa di questi dati
		1. _imaging_
		2. _geofisica_
		3. _signal processing_
	- se consideriamo problemi inversi lineari, li rappresentiamo dal modello lineare $$Ax = y$$
		- dove:
			- $A$ è la matrice che rappresenta il modello, o il sistema di acquisizione;
			- $y$ sono i dati acquisiti dal modello $A$;
			- $x$ sono la causa;
	- nelle applicazioni reali i dati $y$ sono sempre affetti da errore, detto **rumore**, rappresentato dal vettore $\delta$ --> questo perché sono acquisiti da strumenti fisici che presentano per natura un errore, poi devono essere digitalizzati e questo aggiungo rumore
	- allora risolviamo $$Ax = y + \delta = y^{\delta}$$
		- devo gestire gli errori sui dati!
	- la maggior parte dei problemi inversi ha una matrice $A$ mal condizionata, derivata dalla discretizzazione, dalle caratteristiche del sistema di acquisizione
		- quindi anche se $\delta$ è piccolo, la risoluzione del sistema $Ax = y^{\delta}$ darebbe risultati completamente sbagliati
	- allora il problema viene risolto con un problema di minimi quadrati --> questo dà la possibilità di usare una $A$ non quadrata, e poi di tenere conto dell'errore su $y$
		- quindi si risolve $$\min_{x} {\|Ax - y^{\delta}\|_{2}}^{2}$$
		- se la matrice ha rango massimo si possono usare entrambe le strade, ma è sempre consigliata la SVD perché non viene dominata dal mal condizionamento di $A$ e dal rumore di $y$
	- creiamo un problema test
		- il rumore è generato da una gaussiana, come assunzione
		- vediamo come la soluzione naive che usa la SVD (in particolare TSVD) è instabile
		- questo perché la soluzione dei minimi quadrati in realtà è $$\min_{x} {\|Ax - y - \delta\|_{2}}^{2}$$
		- se sviluppiamo con la formula dell'SVD otteniamo due sommatorie: la prima è quella standard, la seconda è quella del rumore ed è quella che genera l'instabilità della soluzione
	- per capire come il rumore influenza la SVD usiamo un grafico che mostra i valori singolari $\sigma_{i}$, i valori $u_{i}^{T}y^{\delta}$ (coefficienti di Fourier) e $\frac{u_{i}^{T}y^{\delta}}{\sigma_{i}}$ (coefficienti della soluzione)
		- le confrontiamo con lo stesso grafico che visualizza solo per $y$ (senza il rumore $\delta$)
		- visualizzando $v_{i}$ (valori singolari destri), si vede proprio che il rumore crea oscillazioni mostruose
		- queste sono le **condizioni discrete di Picard**: quando _i coefficienti di Fourier decrescono più velocemente dei valori singolari_ (indipendentemente dal loro valore)
			- se questo avviene la condizione di Picard è soddisfatta
			- _quando questa non è soddisfatta i coefficienti della soluzione crescono_
		- le oscillazioni derivano quindi da:
			- rumore $\delta$
			- malposizionamento della matrice $A$
	- quindi anche in questo caso **si utilizzano metodi di regolarizzazione**!
		- metodi:
			- **decomposizione in valori singolari troncata** (TSVD)
				- mi fermo quando le condizioni discrete di Picard non sono più soddisfatte
				- quindi la soluzione al problema dei minimi quadrati viene _approssimata_ fermandosi a $K$ iterazioni della sommatoria SVD (non a $n$)
				- a livello di modello posso fare la sommatoria fino a $n$ ma moltiplicando ogni coefficiente per $f_{i}$, dei _fattori di filtro_ che valgono $1$ nelle prime $K$, e $0$ in tutte quelle da $K+1$ a $n$
				- come scelgo $K$? posso guardare il grafico di Picard, e capire che un buon valore di $K$ è quello dopo il quale non sono più soddisfatte le condizioni
			- **regolarizzazione alla Tikhonov**
				- prendo i fattori di filtro $f_{i}$ e li scalo piano piano
				- allora il problema diventa $$\min_{x} {\|Ax - y^{\delta}\|_{2}}^{2} + \lambda {\|Lx\|_{2}}^{2}$$
				- dove $\lambda$ è il parametro di regolarizzazione, mentre $L$ è la matrice identità $I$ oppure una matrice che deriva dalla discretizzazione delle derivate prime o seconde
				- i fattori di filtro sono fatti così: $$f_{i} = \frac{\sigma_{i}^{2}}{\sigma_{i}^{2} + \lambda}$$
				- come scelgo il parametro di Tikhonov? non c'è correlazione con le condizioni di Picard, per cui devo andare a tentativi
				- non esiste un metodo efficiente per scegliere $\lambda$
			- **principio di massima discrepanza**
				- tecnica più utilizzata in pratica
				- bisogna supporre di avere informazioni sul rumore, in particolare la norma $\|\delta\|_{2}$
				- in particolare il parametro $\lambda_{DP}$ deve soddisfare la seguente relazione: $${\|Ax_{\text{tik}} - y^{\delta}\|_{2}}^{2} = \nu {\|\delta\|_{2}}^{2}$$
				- quindi se il residuo è uguale alla norma del rumore per $\nu$, allora quel $\lambda$ è quello ottimale
				- quel $\nu$ è un valore di tolleranza, solitamente $1.01$ o $1.001$

## Domande

## Referenze
