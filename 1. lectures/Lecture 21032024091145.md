---
tags:
  - category/lecture
  - status/ongoing
  - topic/algoritmi-e-strutture-dati
date: 21-03-2024 09:11:45
teacher: pietro.dilena@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- [[Tabella hash]]
	- riassunto delle implementazioni che abbiamo visto per il [[Dizionario]]
	- il dizionario serve per memorizzare ed etichettare gli identificatori (chiavi) nel programma --> i compilatori fanno uso delle _look-up tables_ --> la stringa dell'identificatore diventa la chiave; il valore dell'identificatore diventa il dato
	- le tabelle hash sono una struttura dati che permette un'implementazione estremamente efficiente di strutture dati dizionario
		- idea: generalizzare l'indicizzazione in un array ordinario, ma con qualcosa in più
		- possono avere un costo pessimo lineare, ma in media le prestazioni computazionali sono efficienti --> costo medio $O(1)$
	- nozioni preliminari
		- $U$ = universo di _tutte le chiavi possibili_
		- $K$ = insieme di _tutte le chiavi effettivamente utilizzate_
		- scelte implementative dipendono dal dominio di applicazione
			- se $U$ è molto piccolo ($m$ elementi con $m$ piccolo), allora $|K|$ ~ $|U|$ $\implies$ usiamo tabelle ad indirizzamento diretto
			- se $U$ è molto grande allora $|K| << |U|$ $\implies$ usiamo tabelle hash
		- tabelle ad indirizzamento diretto
			- implementazione basata su array `T` di dimensione $|U|$
			- la chiave $k$ è memorizzata nella posizione $k$ dell'array
			- tutte le chiavi sono distinte
			- pseudo-codice implementativo
			- costo temporale: $O(1)$
			- costo di memoria: $O(T.length)$
				- se $|K|$ ~ $|U|$ soluzione accettabile
				- se $|K| << |U|$ soluzione inaccettabile
				- se consideriamo tutti gli identificatori lunghi massimo 20 caratteri e `T` array di puntatori (4 bytes per puntatore), avremmo che la tabella deve contenere _almeno_ $10^{19}$ Terabytes
					- e stiamo facendo anche una limitazione sul tipo degli identificatori!
		- tabelle hash
			- un array di dimensione $\Theta(|U|)$ richiede troppa memoria se $U$ è grande, allora usiamo un array `T` di dimensione $m$ piccola (proporzionale al numero di chiavi che andremo ad usare, con una stima) e poi usiamo una funzione `hash` che mappa tutte le chiavi dell'universo in un intero compreso tra 0 e $m-1$
			- diciamo allora che $h(k)$ è il valore hash della chiave $k$ (la chiave è per esempio il nome di un identificatore)
				- se scegliamo a caso delle chiavi dell'universo è possibile che `hash` ci dia lo stesso valore per due chiavi diverse! --> in questo caso avviene una collisione --> non può a priori assegnare interi distinti a questi identificatori
				- problema: evitare e gestire le collisioni hash
					- dovremmo minimizzare il più possibile le collisioni --> sono inevitabili
			- ingredienti
				- _funzione `hash`_
					- deve poter essere calcolata velocemente
					- deve garantire una buona distribuzione delle chiavi su `T`
					- una buona distribuzione, infatti, minimizza il rischio di collisioni
				- _metodo per gestire le collisioni_
					- sono inevitabili
					- quando non riusciamo ad evitarle dobbiamo gestirle
				- _array `T` di dimensione $m = \Theta(|K|)$_
					- bisogna scegliere $m$ in modo proporzionale al numero di identificatori che stimiamo ci siano
					- non sappiamo a propri quante chiavi andremo a memorizzare
					- potrebbe essere necessario ridimensionare `T`
					- la scelta migliore di $m$ dipende dalla funzione `hash` e dal metodo usato per gestire le collisioni
			- funzioni hash
				- una buona funzione hash soddisfa (approssimativamente) la proprietà di _uniformità semplice_
					- una funzione hash $h$ deve distribuire uniformemente le chiavi all'interno dell'array `T` negli indici $0, \cdots, m-1$
					- quindi ogni indice dev'essere generato con probabilità $\frac{1}{m}$
					- se alcuni indici sono scelti con maggiore probabilità da $h$ allora avremo un numero maggiore di collisioni!
				- per soddisfare tale proprietà dobbiamo conoscere la distribuzione di probabilità con cui le chiavi sono estratte da $U$
					- conoscere tale distribuzione è irrealistico
					- sono in casi specifici la distribuzione è nota
						- esempio per $U = [0, 1)$ per cui si può fare $h(k) = \lfloor mk \rfloor$
						- questa funzione soddisfa la proprietà di uniformità semplice
				- assunzioni probabilistiche, che concretamente non valgono, ma idealmente sono alla base dell'implementazione di funzioni hash
					1. tutte le chiavi sono equiprobabili
						- tutte le chiavi hanno la stessa probabilità di essere estratte da $U$
						- non sempre vero (esempio identificatori in un programma)
						- semplificazione necessaria per proporre un meccanismo generale
					2. la funzione hash può essere calcolata in tempo $O(1)$
						- una codifica hash non $O(1)$ dominerebbe il costo delle operazioni
						- costerebbe per esempio di più il calcolo dell'hash che la ricerca
						- nella realtà ci accontentiamo di hashing sufficientemente veloci
					3. tutte le chiavi sono valori interi non negativi
						- vero perché possiamo sempre trasformare una qualsiasi chiave $k$ in un intero
							- un numero decimale ottenuto dalla rappresentazione binaria di $k$ può tranquillamente essere interpretato come intero
							- esempio con stringa in cui si codifica ogni carattere
				- metodi
					- metodo della divisione: $$h(k) = k \mod{m}$$
						- ovvero $h(k)$ è il resto della divisione intera di $k$ con $m$
						- vantaggi: molto efficiente, richiede solo una divisione intera
						- svantaggi:
							- suscettibile a specifici valori di $m$ (potrebbe non usare tutto $k$)
							- per esempio per $m = 10$ il valore di $h(k)$ è semplicemente l'ultima cifra di $k$!
							- o ancora peggio se $m = 2^{p}$ allora il valore di $h(k)$ dipende unicamente dagli ultimi $p$ bit (meno significativi) di $k$ e non da tutti i bit di $k$
							- soluzione: dobbiamo scegliere $m$ come _numero primo_ distante da potenze di $2$ e $10$
					- metodo della moltiplicazione: $$h(k) = \lfloor m(kC - \lfloor kC \rfloor) \rfloor$$
						- dove $C$ è una costante $0 < C < 1$
						- moltiplichiamo $k$ per $C$ e prendiamo la parte frazionaria; quindi moltiplichiamo quest'ultima per $m$ e prendiamo la parte intera
						- svantaggi:
							- la costante $C$ influenza la proprietà di uniformità di $h$
							- la costante $C = \frac{\sqrt{5} - 1}{2}$ suggerito da [[Knuth]][^1] statisticamente favorisce la distribuzione delle chiavi --> è stato smontato in seguito
						- vantaggi:
							- $m$ non è critico
					- metodo della codifica algebrica: $$h(k) = (k_{n}x^{n} + k_{n-1}x^{n-1} + \cdots + k_{1}x + k_{0}) \mod{m}$$
						- dove $k_{n}, k_{n-1}, \cdots, k_{i}, \cdots, k_{1}, k_{0}$ è l'$i$-esimo bit della rappresentazione binaria di $k$ o decimale di $k$ o anche il codice ascii dell'i-esimo carattere
						- $x$ è costante
						- vantaggi: dipende da tutti i bit/caratteri della chiave $k$
						- svantaggi: costoso da calcolare --> $\Theta(\log^{2}{k})$, costo polilogaritmico
							- ma attenzione: con la regola di Horner possiamo ridurre questo costo
								- se abbiamo già calcolato $x^{n-1}$ non serve ricalcolare $x^{n}$, basta moltiplicare $x^{n-1} \cdot x$
								- per cui si può riscrivere la formula e otteniamo un costo $\Theta(\log{k})$ --> _in ogni caso non costante_!
								- la funzione `java.lang.String.hashCode()` è basata esattamente sulla codifica algebrica implementata con la regola di Horner, e utilizza i codici ascii dei caratteri
				- problema delle collisioni
					- non siamo mai in grado di eliminarle
						- la probabilità che ci sia una collisione tra due chiavi è sorpredentemente molto alta
							- si pensi al _problema del compleanno_
					- un sacco di attacchi informatici si basano nel crackare tabelle hash
					- soluzioni:
						- concatenamento (_chaining_)
							- se c'è una collisione le chiavi $k$ con lo stesso valore hash $h(k) = i$ sono memorizzate in una lista concatenata (_lista di trabocco_)
							- pseudocodice
							- esempio d'esercizio d'inserimento in tabella hash con concatenamento e funzione hash con metodo della divisione
							- analisi
								- costi:
									- caso ottimo: $O(1)$ per tutte e 3 le operazioni
									- caso pessimo: $\Theta(T[h(k)].length)$ per tutte e 3 le operazioni perché dipendono dalla ricerca lineare sulla lista di trabocco della cella in indice $h(k)$; nota che non dipende dalla dimensione della tabella, ma solo dal numero di elementi che abbiamo in $T[h(k)].length$
									- caso medio: difficile da calcolare, dipende dal numero medio di accessi per cercare una chiave
										- _fattore di carico_ $\alpha = \frac{n}{m}$, ovvero il rapporto tra il numero di elementi nella tabella e la grandezza della tabella stessa
										- sotto l'assunzione di hashing uniforme semplice ogni slot della tabella ha mediamente $\alpha$ chiavi, enunciamo il seguente teorema
											- teorema: una ricerca senza successo in una tabella hash con concatenamento ha costo medio $\Theta(1 + \alpha)$
												- dimostrazione: siamo sotto l'assunzione di hashing uniforme, quindi una chiave $k$ ha uguale probabilità di capitare su uno degli $m$ slot della tabella; se $k$ non compare nella tabella la ricerca visita tutte le chiavi nella lista $T[h(k)]$ che ha in media $\alpha$ chiavi
												- costo medio $\Theta(1 + \alpha)$
											- teorema: una ricerca con successo in una tabella hash con concatenamento ha costo medio $\Theta(1 + \alpha)$
												- anche qui dimostrazione
										- in ogni caso il fattore di carico $\alpha$ influenza il costo
											- $n = O(m) \implies \alpha = O(1) \implies$ costo medio $O(1)$
											- $n = O(m^{2}) \implies \alpha = \Theta(n) \implies$ costo medio $\Theta(n)$
								- dimensione della tabella: 

## Domande

## Referenze
[^1]: _The Art of Computer Programming_