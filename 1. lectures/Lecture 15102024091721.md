---
tags:
  - category/lecture
  - status/pending
  - topic/calcolo-numerico
date: 15-10-2024 09:17:21
teacher: davide.evangelista5@unibo.it
mod: 2
---
# Lezione
---
## Concetti
- approssimazione di dati
	- ripasso
		- quando il grado del polinomio di approssimazione è alto la curva comincia a interpolare il rumore
	- vedremo due modi diversi in cui l'approssimazione può non andare bene:
		- _underfit_, quando la curva risultante è troppo semplice, non cattura la complessità della curva da approssimare (grado troppo basso)
		- _overfit_, quando la curva risultante è esageratamente flessibile, si catturano e quindi amplificano i rumori (grado troppo alto)
	- notazioni:
		- iperparametri, tutti i parametri del modello che devono essere selezionati a mano dall'utente (in questo caso $d$, il grado del polinomio approssimante)
		- parametri, tutti i parametri calcolati dal modello e non modificabili
	- operazione di _tuning_: impostare correttamente gli iperparametri per ottenere un modello che non soffra né di underfit né di overfit
		- per farlo dobbiamo minimizzare il residuo, ma NON che sia uguale a 0 altrimenti vorrebbe dire che abbiamo interpolato il rumore --> dev'essere _medio_
		- la funzione del residuo è $$r(\alpha) = {\|X\alpha - y\|_{2}}^{2}$$
		- se calcoliamo i residui per ogni grado provato, notiamo che al crescere del grado il residuo _si arresta_, per poi crollare dopo un certo grado in poi --> l'**underfit non si verifica se si sceglie l'iperparametro che fa arrestare la decrescita del residuo**
		- per l'overfit non è possibile invece capire a quale grado entra in gioco il rumore
	- regolarizzazione
		- _sparo iperparametri altissimi, introducendo overfit a forza, e poi lo risolvo_
			- per l'overfit la diagnosi non c'è ancora, ma la risoluzione sì!
		- si chiede fondamentalmente al modello di porre il maggior numero possibile di valori (nel nostro caso di $\alpha$) uguale a 0
		- in questo modo è come se non ci fossero e si risolve l'overfit
		- in poche parole si chiede di minimizzare $\alpha$, e si fa modificando i minimi quadrati: $$\min_{\alpha} \frac{1}{2} \|X\alpha  - y\|_{2}^{2} + \frac{\lambda}{2}\|a\|_{2}^{2}$$
			- il primo termine è il fit dei dati, o di fedeltà (risolve i minimi quadrati classico)
			- il secondo termine è di regolarizzazione, perché dopo aver fittato i dati consente al modello di regolarizzare minimizzando $\alpha$
		- si regolano le forze tra i due termini con l'iperparametro $\lambda$, **chiamato termine di regolarizzazione alla Tikhonov** (nella forma in cui è scritto), che regola appunto quando si vuole minimizzare $\alpha$ rispetto a risolvere i minimi quadrati
		- a livello di equazioni normali si ottiene, sempre calcolando il gradiente e ponendolo uguale a 0, una cosa simile al problema dei minimi quadrati: $$X^{T}X\alpha - X^{T}y + \lambda \alpha = 0$$
			- che diventa $$(X^{T}X + \lambda I) \alpha = X^{T}y$$
			- per cui è solo un'estensione dei minimi quadrati
		- test dei vari valori di $\lambda$ con codice Python
		- esercizio suggerito da fare
		- **metodo LASSO**:
			- funzioni di regolarizzazione alternative al termine di Tihkonov
			- guardiamo le [[Insieme di livello|curve di livello]] e notiamo che conviene usare la norma 1 perché c'è più probabilità di ottenere degli 0 in $\alpha$
			- estremamente performante, ma rende il problema intero non derivabile --> non è possibile passare alle equazioni normali
			- per usare il metodo allora bisogna usare **tecniche di ottimizzazione**
- ottimizzazione di dati
	- è una delle applicazioni più importanti dell'algebra lineare
	- infatti tutti i problemi reali risolvibili computazionalmente possono essere in qualche modo riformulati come problemi di ottimizzazione, partendo dalla Ricerca Operativa fino all'ingegneria
	- tutti possono essere riscritti come $$\min_{x \in \Omega} f(x)$$
	- dove $f: \Omega \to \mathbb{R}$, e $\Omega$ è l'insieme dei vincoli che $x$ deve rispettare
	- un esempio è il _problema del commesso viaggiatore_, modellizzabile matematicamente in $$\min_{x \in \Omega \subset \mathbb{R}^{n}} \sum\limits_{i=1}^{n-1} d(x_{i}, x_{i+1})$$
	- classificazione dei problemi di ottimizzazione
		- $\Omega = \mathbb{R}^{n}$, problema svincolato (_unconstrained_)
		- $\Omega \subset \mathbb{R}^{n}$, problema vincolato (_constrained_)
			- $\Omega \subseteq \mathbb{Z}^{n}$, problema discreto (sottocaso dei vincolati)
	- noi guardiamo gli _svincolati_
	- altra classificazione, su $f(x)$
		- $f \in C^{1}(\Omega)$, ossia è almeno derivabile, il problema è detto _smooth_
		- $f \notin C^{1}(\Omega)$, ossia non è derivabile, il problema è detto _non-smooth_ (difficilissimi da risolvere)
	- ulteriore classificazione su $f(x)$
		- $f(x) = x^{T}w$, quindi è lineare in $x$, il problema è detto _lineare_
		- $f(x) = x^{T}Ax + x^{T}b$, quindi è una funzione quadratica in $x$, allora il problema è detto _quadratico_
	- noi guardiamo solo funzioni _smooth_
	- spesso usiamo come funzione $f(x)$ la seguente $$f(x) = (x-1)^{2} + e^{x}$$
	- codice in Python
	- ripassare i gradienti

## Domande

## Referenze
