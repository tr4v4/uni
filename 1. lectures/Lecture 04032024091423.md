---
tags:
  - category/lecture
  - status/finished
  - topic/algoritmi-e-strutture-dati
date: 04-03-2024 09:14:23
teacher: pietro.dilena@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- [[Algoritmo di ordinamento]]
	- problema tipico della programmazione
		- consiste nel trovare una [[Permutazione|permutazione]] di un array t.c. i numeri sono ordinati in ordine crescente (o decrescente)
	- ci sono moltissimi algoritmi che implementano una soluzione
		- più di una cinquantina
		- mondo molto vasto
	- divisione
		- _incrementali_: selection sort, insertion sort
		- _divide et impera_: merge sort, quick sort
		- _non-comparativi_: counting sort, radix sort
	- ma perché non studiamo semplicemente l'algoritmo migliore?
		- non possiamo individuare a priori un algoritmo che avrà sempre le performance migliori
		- dipende dai singoli casi in cui capita l'array in input
		- **non esiste l'algoritmo di ordinamento più efficiente**, perché dipende dall'input
	- nozioni preliminari
		- ogni elemento dell'array che vogliamo ordinare contiene
			- _chiave_
			- _valore_, associato alla chiave
		- noi riordiniamo l'array rispetto alla chiave, non rispetto al valore
			- attenzione chiave $\neq$ indice, noi ordiniamo array di solo chiavi
			- potremmo avere un array di interi, che sono chiavi, e i valori potrebbero essere stringhe associate a tali chiavi: se ordini le chiavi ordini i valori
		- proprietà
			- _in place_: l'algoritmo riordina gli elementi nell'array in input, senza nessun array aggiuntivo per il calcolo
			- _stabilità_: un algoritmo di ordinamento si dice stabile se dato in input un array di chiave-valore, valori con la stessa chiave appaiono nell'array ordinato nello stesso ordine in cui appaiono nell'array in input
				- questa definizione vale ovviamente per array che contengono chiave-valore, nel nostro caso gli array contengono solo chiavi, ma se volessimo ordinare un array contenente oggetti più complessi avremmo bisogno di definire una chiave secondo cui ordinare tale oggetti: non ha molto senso negli algoritmi di ordinamento di interi, ha più senso negli [[Tabella hash|hashmap]]
				- algoritmi non stabili possono sempre essere trasformati in stabili
					- usiamo come chiave la coppia (_key_, _index_)
						- _index_ è la posizione del valore nell'array in input
						- _key_ è la chiave primaria di ordinamento, _index_ la secondaria
						- esempio: (10, 2) < (10, 6)
	- ripasso su [[Array|array]]
		- struttura dati costituita da una sequenza di **elementi omogenei** accessibili tramite un indice
		- hanno anche una dimensione fissa, infatti la loro allocazione consiste in una porzione contigua di byte
			- si può ridimensionare un array, riallocandolo in una posizione con più spazio intorno --> questo processo ha un costo non costante
		- per le sue proprietà il costo di accesso è costante per tutti gli elementi
	- algoritmi
		- algoritmi di ordinamento **incrementali**
			- assumendo di partire da un prefisso di primi $k$ elementi ordinati nell'array $A$, questi algoritmi estendono questo prefisso a un prefisso di $k+1$ elementi, andando quindi a inserire un nuovo elemento in modo ordinato nel prefisso
			- due implementazioni
				- **selection sort** --> cerca il minimo nel postfisso e lo aggiunge al prefisso
					- ad ogni passo $i = 1, \cdots, n-1$
						1. cerca la posizione $j$ della minima chiave $A[i, \cdots, n]$
						2. swap tra $A[j]$ e $A[i]$
					- autore sconosciuto! Si sa che viene dagli anni 50
					- algoritmo _non stabile_
						- esempio delle due chiavi 7 che non presentano lo stesso ordine tra l'inizio e la fine dell'ordinamento
					- analisi della [[Complessità computazionale|complessità computazionale]]
						- funzione `swap` ha costo costante
						- ha sempre lo stesso costo, nel caso ottimo, pessimo e medio
						- ciclo interno, della ricerca del minimo, è $\Theta(n - i)$ volte, e viene eseguito $n-1$ volte
							- quindi $(n-1) + (n-2) + \cdots + 1 = \sum\limits_{i=1}^{n-1} n - i = \sum\limits_{i=1}^{n-1} i = \frac{n(n-1)}{n} = \Theta(n^{2})$
				- **insertion sort** --> inserisce mano a mano un elemento nell'array ordinato in modo ordinato
					- ad ogni passo $i = 2, \cdots, n$
						1. $A[1, \cdots, i-1]$ è ordinato
						2. inserisci $A[i]$ nella posizione corretta $A[1, \cdots, i]$
					- intuitivamente, approccio utilizzato per ordinare carte da gioco
					- autore John Mauchly, 1946
					- algoritmo _stabile_
					- analisi
						- il ciclo `for` viene eseguito sempre interamente, mentre il ciclo `while` potrebbe non essere eseguito --> questo ci fa intuire come i casi di studio si diversifichino
						- quindi
							- _caso ottimo_: array già ordinato, il ciclo `while` non viene proprio eseguito, $\Theta(n)$
								- in realtà ci sono diversi casi
								- se prendiamo di un array ordinato 5 numeri e li mettiamo in posizioni scorrette (tipo in fondo), avremo $\Theta(n)$ (ciclo for) + $O(nk)$ (ciclo while) = $O(nk)$, quindi ha un costo lineare per una $k$ costante
									- l'idea è allora, _nonostante l'algoritmo abbia un caso pessimo quadratico_ (si veda dopo), **il caso ottimo è più frequente del caso pessimo**, questo perché _array quasi ordinati sono più frequenti di array completamente disordinati_ (in ordine decrescente)
										- in definitiva l'insertion sort, nella maggior parte dei casi, ha un costo complessivo lineare
							- _caso pessimo_: array in ordine decrescente, il ciclo `while` viene eseguito sempre il numero massimo di volte, $\Theta(n^{2})$
								- $\sum\limits_{i=2}^{n} (i-1) = \sum\limits_{i=1}^{n-1} i = \frac{n(n-1)}{2} = \Theta(n^{2})$
							- _caso medio_: il numero medio di iterazioni del ciclo `while`, numero minimo è 0 volte, il numero massimo è $i$ volte, quindi in media sarà $\frac{i}{2}$ volte
								- per cui avremo $\frac{2}{2} + \frac{3}{2} + \cdots + \frac{n}{2} = \sum\limits_{i=2}^{n} \frac{i - 1}{2} = \Theta(n^{2})$
		- algoritmi **divide et impera**
			- strategia per ottenere controllo militare e politico: fare in modo che gli avversari siano divisi e si combattano tra di loro
			- introdotta da [[Giulio Cesare]]
			- l'idea è:
				1. dividere il problema in sottoproblemi più piccoli;
				2. risolvere tali sottoproblemi;
				3. combinare le soluzioni.
			- facili da implementare come algoritmi [[Ricorsione|ricorsivi]]
			- generalmente _più efficienti degli algoritmi incrementali_
			- **merge sort**
				- autore [[John Von Neumann]][^1], del 1945
				- fasi
					1. _divide_
						- dividiamo $A[1, \cdots, n]$ in due metà $A_{1} = A[1, \cdots, q]$ e $A_{2} = A[q+1, \cdots, n]$ dove $q = \lfloor \frac{1+n}{2} \rfloor$
					2. _conquer_
						- dobbiamo ordinare i due array $A_{1}$ e $A_{2}$
						- quindi richiamiamo lo stesso merge sort fino ad ottenere $A_{1}$ e $A_{2}$ di due soli elementi
					3. _combine_
						- combiniamo i due array ordinati $A_{1}$ e $A_{2}$ in un unico array ordinato
				- algoritmo _stabile_, la fase di `merge` preserva la stabilità
				- algoritmo _non in-place_, perché utilizza nella fase di `merge` un array di appoggio
					- trasformarlo _in-place_ è possibile ma lo rende complicatissimo e meno efficiente
				- il `merge` è la parte più complessa, critica
					- il `<=` è quello che preserva la stabilità
				- analisi complessità
					- guardiamo prima il `merge` --> ha un costo lineare, $\Theta(n)$, perché indipendentemente da quali sono i numeri, gli indici dovranno scorrere tutto l'array, e alla fine dovremmo ricopiare l'array di appoggio nell'array originale
					- ora facciamo l'[[Equazione di ricorrenza|equazione di ricorrenza]] per applicare quindi il [[Master Theorem|master theorem]]
						- $$T(n) = \begin{cases} 1 & n \leq 1 \\ 2T(\frac{n}{2}) + n & n > 1 \end{cases}$$
						- che ci fa venire $$\Theta(n \log{n})$$
					- tale costo coincide per caso ottimo, pessimo e medio
			- **quick sort**
				- fasi
					- divide: partizioniamo l'array in due sottoarray sulla base di un _pivot_ scelto (noi prenderemo sempre l'ultimo)
					- conquer: richiamiamo ricorsivamente l'algoritmo sui sottoarray
					- combine: non c'è bisogno di unire gli array, perché sono già ordinati
				- autore Tony Hoare, 1959
					- lo fece come tesi di dottorato!
				- algoritmo _non stabile_, e _in-place_
					- il partizionamento può essere fatta anche _non in-place_ per renderlo più semplice
				- analisi
					- il partizionamento ha costo lineare, $\Theta(n)$
					- _costo computazionale dipende da quanto bene `partition` riesce a partizionare l'array $A$ nei due sotto-array_
					- il caso medio è mostruosamente complesso (una volta veniva dimostrato con il [[Metodo della sostituzione]])
					- infatti se scriviamo l'equazione di ricorrenza la dimensione dei due sottoarray dipende unicamente da $q$, ovvero dal pivot scelto
						- ma non sappiamo quanto sia centrale!
					- intanto partizionamento in _caso pessimo_
						- ogni volta che il pivot lo scegliamo o minimo o massimo numero dell'array ci capita che il partizionamento non partiziona!
							- abbiamo un **partizionamento sbilanciato**
						- l'equazione di ricorrenza è ora possibile scriverla, e risolvendola con il metodo iterativo ci viene $\Theta(n^{2})$
						- e il bello è che questo avviene anche questo l'array è già ordinato (perché se scegliamo l'ultimo elemento questo è il massimo), o completamente disordinato (decrescente) (perché scegliamo l'ultimo elemento come pivot, che è il minimo)
						- ma attenzione: si può migliorare
							- il problema è la scelta del pivot --> non appena scegliamo qualcosa di fisso vengono fuori casistiche che, come in questo caso, sono pessimi
							- un modo sarebbe quello di scegliere randomicamente il pivot --> questo aumenta la probabilità di avere partizioni bilanciate, o meglio, di non avere partizioni sempre sbilanciate
					- _caso ottimo_
						- ogni volta che scegliamo un pivot otteniamo un **partizionamento bilanciato**
						- risolvendo l'equazione di ricorrenza associata otteniamo $\Theta(n \log{n})$
					- _caso medio_
						- equazione di ricorrenza $$T(n) = \begin{cases} 1 & n \leq 1 \\ T(i) + T(n-i-1) + n & n > 1 \end{cases}$$
							- dove $i$ è la lunghezza del primo sottoarray
							- e quindi questo $i$ può cambiare ad ogni chiamata ricorsiva
						- allora assumiamo probabilisticamente che le partizioni siano tutte _equiprobabili_, per cui facciamo una media dei casi, e otteniamo
							- $$T(n) = \begin{cases} 1 & n \leq 1 \\ \frac{1}{n} \sum\limits_{i=0}^{n-1} [T(i) - T(n-i-1)] + n & n > 1 \end{cases}$$
						- con una serie di dimostrazioni otteniamo $T(n) = O(n \log{n})$
	- algoritmi in [[Java]]
		- nella classe `java.utils.Arrays` metodi di ordinamento
			- il metodo statico `sort` è una combinazione dell'insertion sort, merge sort e quick sort
			- il quick sort implementato usa due pivot, per diminuire la probabilità di beccarne uno pessimo, _DualPivot QuickSort_
			- l'insertion sort è usato su istanze piccole, $\leq 47$
				- spesso è usato nei `merge` e `partition` 
			- il merge sort è preferito al quick sort su istanze grandi, $\geq 286$
			- Python usa il Timsort
	- tabella riassuntiva degli algoritmi analizzati
		- curiosità sull'insertion sort, che è l'unico in grado di essere eseguito _in-live_, ovvero man mano che inserisco un elemento nell'array

## Domande

## Referenze
[^1]: ovviamente lo stesso dell'[[Architettura di von Neumann|architettura]]