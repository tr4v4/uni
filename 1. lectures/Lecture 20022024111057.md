---
tags:
  - category/lecture
  - status/finished
  - topic/algoritmi-e-strutture-dati
date: 20-02-2024 11:10:57
teacher: pietro.dilena@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- continuo [[Lecture 19022024090000|lezione precedente]]
	- algoritmo 2
		- calcolati il lower e l'upper bound dell'algoritmo 2, ricorsivo, per calcolare l'$n$-esimo numero di Fibonacci otteniamo $$2^{\frac{n}{2}} \leq T(n) \leq 2^{n}$$
		- ovvero un range di tempo d'esecuzione approssimato ma sufficiente per capire se l'algoritmo è lento o veloce
			- steps
				- assumiamo che $K$ sia uguale al numero di chiamate ricorsive effettuate in 1 secondo
				- settiamo $K$ a $1.000.000$, per esempio, e vogliamo calcolare il 100° numero di Fibonacci
				- sappiamo allora che il tempo in secondi è $\geq \frac{2^{\frac{n}{2}}}{1000000} sec$
				- per il 100° numero di Fibonacci, con queste condizioni, ci si mette almeno 36 anni!
				- settiamo allora $K=100.000.000$, ci vengono fuori 3 mesi di calcolo...
				- con questo grande miglioramento di performance abbiamo ridotto molto il tempo d'esecuzione, MA: se volessimo calcolare anche solo il 200° numero di Fibonacci, con le stesse condizioni, ci vorrebbero 400 miliardi di millenni
			- questo procedimento ci fa capire come l'algoritmo ricorsivo sia inutilizzabile
		- il problema dell'algoritmo consiste nel calcol ripetuto di numeri già calcolati in precedenza
	- algoritmo 3
		- invece che ricalcolare ogni volta i numeri andiamo a memorizzare i valori salvati di mano in mano, in questo modo
			- usiamo un array inizializzato a $F[1] = 1$ e $F[2] = 1$[^1], facciamo un ciclo da 3 a $n$ e mettiamo nella posizione $i$ la somma dei numeri nelle due celle precedenti
			- il risultato finale sarà in $F[n]$
		- calcoliamo
			- tempo di calcolo --> contiamo il numero di operazioni elementari
				- conteggio
					- 1 creazione
					- 1 assegnamento
					- 1 assegnamento
					- n-2 assegnamenti (ciclo for)
					- n-2 assegnamenti + somme
					- 1 return
				- totale: numero complessivo di operazioni elementari = $4 + (n-2) + (n-2) + (n-2) = 4 + 3(n-2) = 3n-2$
					- ovvero, il tempo di calcolo è proporzionale a $n$
			- memoria usata --> sommiamo la dimensione delle variabili usate
				- conteggio
					- $n$
					- $F[n]$
					- $i$
				- totale: $1 + n + 1 = n + 2$
					- ovvero, memoria usata proporzionale a $n$
		- il problema di questo algoritmo sta nel fatto che non ci è necessario memorizzare tutti gli $n$ precedenti numeri di Fibonacci
	- algoritmo 4
		- quello classico --> vengono utilizzate solo due variabili, $a=1$ e $b=1$, quindi si fa
			- operazioni nel ciclo for
				- $c = a+b$
				- $a = b$
				- $b = c$
			- e si ripete per $n-2$ volte
			- alla fine il risultato sarà in $b$
		- calcoliamo
			- tempo di calcolo
				- conteggio
					- 1 assegnamento
					- 1 assegnamento
					- n-2 assegnamenti
					- n-2 assegnamenti + somma
					- n-2 assegnamenti
					- n-2 assegnamenti
					- 1 return
				- totale: $1 + 1 + n-2 + n-2 + n-2 + n-2 + n-2 + 1 = 3 + 5(n-2) = 5n - 7$
					- quindi, tempo di calcolo proporzionale a $n$
			- memoria usata
				- totale: usiamo solo 5 variabili
					- quindi _costante_
	- attenzione: confrontiamo l'algoritmo 3 e il 4
		- notiamo che su grandi numeri, nonostante il 3 faccia $3n - 2$ operazioni e il 4 $5n - 7$, il 4 è più veloce!
			- ci sono una serie di motivi che provocano, per entrambi gli algoritmi, l'inutilità delle costanti calcolate per il tempo di calcolo
			- sostanzialmente ci basta sapere che entrambi gli algoritmi hanno un tempo di calcolo proporzionale a $n$
		- inoltre l'algoritmo 3, sempre su grandi numeri, va a occupare troppa memoria
			- quindi la differenza importante sta nella memoria
	- algoritmo 5
		- teorema su una particolare matrice binaria che elevata alla $n-1$ produce l'$n$-esimo numero di Fibonacci
		- lo sfruttiamo per creare un algoritmo molto semplice, sempre iterativo
		- calcoliamo
			- tempo di calcolo
				- contiamo
					- n-1 moltiplicazioni di matrici
					- 2n-1 assegnamenti
					- 1 return
				- totale: tempo di calcolo proporzionale a $n$, facciamo solo una stima, andando a trascurare effettivamente l'implementazione di, per esempio, un _dot-product_
			- memoria usata
				- contiamo
					- $A$
					- $i$
					- $n$
				- totale: indipendente da $n$, costante
					- quindi nessun miglioramento rispetto all'algoritmo 4
		- MA: è un punto di partenza per l'algoritmo finale
	- algoritmo 6
		- possiamo velocizzare di molto le operazioni di moltiplicazioni di matrici --> tecnica chiamata _exponentiation by squaring_[^2]
		- è ricorsivo
		- il tempo di calcolo è proporzionale a $\log_{2}{n}$
		- così come anche la memoria usata, proporzionale a $\log_{2}{n}$
		- **costo logaritmico è buonissimo**
	- riassunto in **notazione asintotica**
		- ![[confronto-algoritmi.png]]
	- valore di input vs. dimensione dell'input
		- dobbiamo calcolare il costo rispetto alla dimensione dell'input, e non al valore
		- la dimensione è in binario, perché l'algoritmo è eseguito su computer
		- la dimensione di un intero in binario è logaritmica rispetto alla base 2, quindi si ha $|n| = \lceil \log_{2}{n} \rceil$, per cui la vera, formale, notazione asintotica dovrebbe essere in dimensione dell'input
- lezione
	- [[Notazione asintotica|notazione asintotica]]
		- scopo: analizzare il tempo di calcolo e l'occupazione degli algoritmi in termini di dimensione dell'input
		- buona misura per tempo di calcolo e memoria usata?
			- tempo di calcolo --> secondi? No
			- memoria usata --> MB? No
			- perché entrambi dipendono dalla macchina e dal linguaggio
			- studiamo allora il comportamento asintotico dell'algoritmo come misurarne tempo e memoria
		- comportamento asintotico di un algoritmo
			- ignora costanti additive/moltiplicative e termini di ordine inferiore
			- descrive quanto velocemente tempo/memoria crescono rispetto alla dimensione dell'input
				- non il tempo/memoria precisa --> ma il **rate di crescita** rispetto alla dimensione dell'input
			- utilizziamo **funzioni di costo**
				- definizione: tutte le [[Funzione matematica|funzioni matematiche]] positive, ovvero che prendono un input positivo e restituiscono un output positivo
			- motivo del perché ci interessa il rate di crescita
				- supponiamo di avere due algoritmi con le seguenti funzioni di costo per il tempo di calcolo
					- $f_{A}(n) = 10^{2}n$
					- $f_{B}(n) = 10^{-2}n^{2}$
				- quale algoritmo ha prestazioni migliori? il più veloce?
				- ![[confronto-comportamento-asintotico.png]]
		- definizione di $O$-grande[^3]
			- data una funzione di costo $g(n)$ definiamo l'insieme di funzioni per cui $g(n)$ rappresenta un limite asintotico superiore come
				- $$O(g(n)) = \{f(n) | \exists c > 0, n_{0} \geq 0 : \forall n \geq n_{0}, f(n) \leq cg(n)\}$$
				- con abuso di notazione diciamo $f(n) = O(g(n))$ per dire $f(n) \in O(g(n))$
			- esempio di dimostrazione della notazione asintotica tra due funzioni di costo (non viene chiesta all'esame)

## Domande

## Referenze
[^1]: attenzione: gli indici nello pseudo-codice partono da 1
[^2]: letteralmente "esponenziamento dal quadratando"
[^3]: una generalizzazione dell'[[O-piccolo]]