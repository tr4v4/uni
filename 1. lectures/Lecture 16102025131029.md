---
tags:
  - category/lecture
  - status/pending
  - topic/introduzione-all-apprendimento-automatico
date: 16-10-2025 13:10:29
teacher: andrea.asperti@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- **naive bayes gaussiano**
	- si suppone la distribuzione normale perché è la distribuzione che ha entropia maggiore
	- sulle ascisse c'è la variabile aleatoria continua
	- sulle ordinate c'è la distribuzione, data dalla gaussiana
	- esempio con peso dell'animale sulle ascisse, gatto e cane
		- il gatto pesa ha media del peso minore e ha una varianza più bassa
		- viceversa per il cane
	- definite le due gaussiane, definiamo una soglia discreta di classificazione
		- questo fa si che ci siano degli errori nella classificazione!
			- ![[naive-bayes-gaussiano-errori.png]]
			- il falso negativo, per esempio, è un cane che viene classificato come gatto
			- il falso positivo è invece un gatto che viene classificato come cane
		- true positive e true negative è quando il modello ci prende
		- contrario con false
		- metriche
			- $accuratezza = \frac{TP + TN}{ALL}$
			- $precisione = \frac{TP}{TP + FP}$, quanto è preciso nella classificazione dei positivi
			- $richiamo = \frac{TP}{TP + FN}$, che percentuale dei positivi reali ho preso
			- $F1 = 2\frac{Precision \cdot Recall}{Precision + Recall}$, media armonica tra precisione e richiamo
		- spostamento della soglia di classificazione per aumentare la precisione --> diminuisce il richiamo!
	- funzionamento
		- assumiamo che per ogni valore $y_{k}$ di $Y$ la variabile aleatoria $\mathbb{P}(X_{i}|Y = y_{k})$ abbia una distribuzione normale $$\mathcal{N}(x|\mu_{ik}, \sigma_{ik})$$
		- l'apprendimento consiste nello stimare quindi $\mu_{ik}, \sigma_{ik}$ e $\pi_{k} = \mathbb{P}(Y = y_{k})$
		- la classificazione è $$Y^{new} = \arg\max_{k} \pi_{k} \cdot \prod_{i} \mathcal{N}(a_{i} | \mu_{ik}, \sigma_{ik})$$
		- per fare le stime usiamo sempre [[MLE]]
- sempre approccio probabilistico, machine learning tradizionale: **regressione logistica**
	- ogni neurone all'interno di una rete neurale fa una regressione logistica
	- idea
		- naive bayes di permette di calcolare $\mathbb{P}(Y|X)$ sapendo $\mathbb{P}(Y)$ e $\mathbb{P}(X|Y)$
			- è un approccio generativo --> stiamo cercando di stimare i dati di input data la categoria
		- perché non possiamo cercare di apprendere direttamente $\mathbb{P}(Y|X)$?
			- non è così ovvio...
			- quello che fanno i modelli è partire da una classe e ottimizzare i parametri
			- per farlo dovremo cercare di capire come possiamo esprimere parametricamente la probabilità $\mathbb{P}(Y|X)$
			- un modo per farlo è cercare di dare una definizione matematica di come potrebbe essere fatta questa distribuzione, parametrica!
	- troviamo questa forma!
		- supponiamo che $Y$ sia una variabile aleatoria booleana
		- supponiamo che $X_{i}$ siano continue, indipendenti e abbiano distribuzioni gaussiane e che la varianza dei dati non dipenda dalla categoria (per ogni categoria la varianza è la stessa)
		- supponiamo che $Y$ abbia una distribuzione di Bernoulli ($\pi$)
		- allora vale
			- $\mathbb{P}(Y=1 | X = (x_{1}, \cdots, x_{n})) = \frac{1}{1 + e^{w_{0} + \sum\limits_{i}w_{i}x_{i}}}$
			- questo ci consente di classificare un certo $X$ semplicemente guardando se $w_{0} + \sum\limits_{i}w_{i}x_{i} > 0$
			- è un modello lineare!
	- la regressione logistica assume proprio che $\mathbb{P}(Y = 1 | X = (x_{1}, \cdots, x_{n})) = \frac{1}{1 + \exp(-w_{0} - \sum\limits_{i}w_{i}x_{i})}$ e cerca di stimare i parametri $w_{i}$
		- la funzione $\sigma(x) = \frac{1}{1 + e^{-x}}$ si chiama **funzione logistica**, o **sigmoide**
		- trasforma un valore numerico in una probabilità!
	- adesso che abbiamo il modello e i parametri che lo caratterizzano, dobbiamo capire come ottenere i valori dei parametri $w$ che massimizzano la probabilità --> MLE
		- troviamo la formula della likelihood, in questo modo capiamo anche come confrontare i modelli...
		- esiste, ma non ha una soluzione analitica!
		- il nostro obiettivo è trainnare quindi il modello, e non potendo farlo calcolando la formula chiusa della likelihood, facciamo la tecnica iterativa detta [[Metodo di discesa del gradiente]]

## Domande
- mmmmmmmmmh

## Referenze
