---
tags:
  - category/lecture
  - status/finished
  - topic/algebra-e-geometria
date: 24-04-2024 09:20:14
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- prodotto scalare
	- sul libro si chiama _prodotto scalare definito positivo_ (_euclideo_), vedere un altro libro per riferimenti più accurati di _Alessandro Savo, Geometria_
	- **prodotto scalare tra due vettori nel piano**
		- immaginiamo un vettore nel piano $\mathbb{R}^{2}$ del tipo $v = (x_{1}, y_{1})$; l'angolo $\theta$ si crea tra $v$ e la semiretta positiva dell'asse $x$
		- siano $v = (x_{1}, y_{1}), w = (x_{2}, y_{2})$, allora il prodotto scalare $<v, w>$ è definito come $$<v, w> = x_{1}x_{2} + y_{1}y_{2} \in \mathbb{R}$$
		- la notazione si può confondere con lo spazio generato, si può quindi indicare come $v \cdot w$
		- osservazione: $<v, w> = |v| \cdot |w| \cdot \cos{\theta}$ dove $\theta$ è l'angolo convesso formato dalle semirette di $v, w$
			- immagine
			- dimostrazione:
				- sia $v = (x_{1}, y_{1})$ e $w = (x_{2}, y_{2})$ con $\theta = \theta_{2}-\theta_{1}$ (si veda immagine)
				- allora $<v, w> = x_{1}x_{2} + y_{1}y_{2} = |v|\cos{\theta_{1}}|w|\cos{\theta_{2}} + |v|\sin{\theta_{1}}|w|\sin{\theta_{2}} = |v||w| (\cos{\theta_{1}}\cos{\theta_{2}} + \sin{\theta_{1}}\sin{\theta_{2}}) = |v||w|\cos{(\theta_{2}-\theta_{1})}$
		- per cui $v, w$ sono perpendicolari $\iff \theta = \frac{\pi}{2} \iff \cos{\theta} = 0 \iff <v, w> = 0$
	- **prodotto scalare in $\mathbb{R}^{n}$**
		- è una funzione $\mathbb{R}^{n} \times \mathbb{R}^{n} \to \mathbb{R}$ definita come $(v, w) \to <v, w>$
		- se $v = (x_{1}, \cdots, x_{n}), w = (y_{1}, \cdots, y_{n})$ allora $<v, w> = x_{1}y_{1} + \cdots x_{n}y_{n}$
		- proprietà
			1. $<x, y> = <y, x>$ --> è una funzione simmetrica;
			2. $<\lambda x, y> = \lambda <x, y>$
			3. $<x+y, z> = <x, z> + <y, z>$
			4. $<x, x> \geq 0 \ \ \forall x \in \mathbb{R}^{n}$
			5. $<x, x> = 0 \implies x = \underline{0}$
				- dimostrazione: $<x, x> = x_{1}x_{1} + \cdots + x_{n}x_{n} = x_{1}^{2} + \cdots x_{n}^{2} = 0 \iff x_{1} = \cdots = x_{n} = 0$
				- equivalentemente si dimostra la proprietà 4
		- per le proprietà 2 e 3 si dice che il _prodotto scalare è bilineare_
	- **norma di $x \in \mathbb{R}^{n}$** (è il modulo)
		- $$||x|| = \sqrt{<x, x>} = \sqrt{x_{1}^{2} + \cdots + x_{n}^{2}}$$
		- se $n = 2$ allora $||x|| = |x|$, modulo standard di un vettore, ovvero la sua lunghezza $= \sqrt{x_{1}^{2} + x_{2}^{2}}$
		- per $n = 1$ si ha proprio $|x| = \sqrt{x \cdot x} = ||x||$
	- **disuguaglianza di Schwarz**
		- $$|<x, y>| \leq ||x|| \cdot ||y||$$
		- se $x \neq \underline{0} \neq y$ otteniamo che $$0 \leq \frac{|<x, y>|}{||x|| \cdot ||y||} \leq 1$$
			- dove possiamo indicare $\frac{|<x, y>|}{||x|| \cdot ||y||} = a$, e ogni numero $a$ compreso tra 0 e 1 è il coseno di un angolo!
			- per cui $x, y$ formano un angolo $\theta$ dove $$\cos{\theta} = a$$
	- **ortogonalità**
		- due vettori $x, y \in \mathbb{R}^{n}$ si dicono ortogonali (o perpendicolari) se $<x, y> = 0$
		- osservazione: se $v \in \mathbb{R}^{n}$ ha la proprietà che $<v, w> = 0 \ \ \forall w \in \mathbb{R}^{n}$ allora $v = \underline{0}$, ossia _l'unico vettore di $\mathbb{R}^{n}$ perpendicolare a tutti gli altri è il vettore nullo_
	- **base ortonormale**
		- i vettori $v_{1}, \cdots, v_{n}$ si dicono ortonormali se $<v_{i}, v_{j}> = \delta_{ij} = \begin{cases}1 & i = j \\ 0 & i \neq j\end{cases}$
			- nota: $\delta_{ij}$ si chiama _delta di ..._
		- cioè $||v_{i}|| = 1 \ \ \forall i = 1, \cdots, n$ e $v_{i}$ è perpendicolare a $v_{j}$ se $i \neq j$
		- esempi:
			- $e_{1}, e_{2}$ di $\mathbb{R}^{2}$
			- ma anche $e_{1}, e_{2}, e_{3}$ per $\mathbb{R}^{3}$
		- in generale $C = \{e_{1}, \cdots, e_{n}\}$ è base ortonormale di $\mathbb{R}^{n}$
		- proposizione:
			- siano $v_{1}, \cdots, v_{k} \in \mathbb{R}^{n}$ vettori non nulli a due a due ortogonali, allora $v_{1}, \cdots, v_{k}$ sono linearmente indipendenti
			- dimostrazione:
				- $\lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n} = \underline{0} \iff \lambda_{1} = \cdots = \lambda_{n} = 0$
				- consideriamo $<\underline{0}, v_{1}> = <\lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n}, v_{1}> \stackrel{bilinearità}{=} \lambda_{1}<v_{1}, v_{1}> + \cdots + \lambda_{n}<v_{n}, v_{1}> = \lambda_{1}<v_{1}, v_{1}> = 0 \iff \lambda_{1} = 0$ (perché $<v_{1}, v_{1}> \neq 0$ in quanto i vettori sono non nulli)
				- analogamente si ripete il procedimento per un generico $v_{i}$
			- corollario: il massimo numero di vettori non nulli di $\mathbb{R}^{n}$ perpendicolari tra loro è $n$
		- esempio
			- $v = (3, 1, -8)$, e faccio $<v, e_{1}> = 3$, e $<v, e_{2}> = 1$, $<v, e_{3}> = -8$
			- per cui si ha che le coordinate di $v$ rispetto alla base canonica $c$ $$(v)_{c} = (<v, e_{1}>, <v, e_{2}>, <v, e_{3}>)$$
		- proposizione: sia $\beta = \{v_{1}, \cdots, v_{n}\}$ una base ortonormale di $\mathbb{R}^{n}$, allora $(v)_{\beta} = (<v, v_{1}>, \cdots, <v, v_{n}>)$
			- dimostrazione: $v = \lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n}$, allora $<v, v_{i}> = <\lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n}, v_{i}> \stackrel{bilinearità}{=} \lambda_{1}<v_{1}, v_{i}> + \cdots + \lambda_{i}<v_{i}, v_{i}> + \cdots + \lambda_{n}<v_{n}, v_{i}> = \lambda_{1} \cdot 0 + \cdots + \lambda_{i} \cdot 1 + \cdots + \lambda_{n} \cdot 0 = \lambda_{i}$
	- **sottospazi ortogonali**
		- definizione: sia $W \leq \mathbb{R}^{n}$, allora $W^{\bot} = \{v \in \mathbb{R}^{n} | <v, w> \ \ \forall w \in W\}$; questo è un sottospazio di $\mathbb{R}^{n}$ e si dice il sottospazio ortogonale a $W$
			- dimostrazione sottospazio: da fare per casa
		- proposizione: se $W = \langle w_{1}, \cdots, w_{k} \rangle$ allora $W^{\bot} = \{v \in \mathbb{R}^{n} | <v, w_{i}> = 0 \ \ \forall i = 1, \cdots, k\}$
			- fondamentalmente per controllare se $v \in W^{\bot}$ basta verificare la perpendicolarità tra $v, w_{i}$ per ogni $w_{i}$ che genera $W$
			- dimostrazione:
				- se $v \in W^{\bot}$ allora $<v, w_{i}> = 0 \ \ \forall i = 1, \cdots, k$, quindi $W^{\bot} \subseteq \{v \in \mathbb{R}^{n} | <v, w_{i}> = 0 \ \ \forall i = 1, \cdots, k\}$
				- ora supponiamo $<v, w_{i}> = 0 \ \ \forall i = 1, \cdots, k$ e sia $w$ un qualsiasi vettore di $W$ mostriamo che $<v, w> = 0$
				- abbiamo $w = \lambda_{1}w_{1} + \cdots + \lambda_{k}w_{k}$, allora $<v, w> = <v, \lambda_{1}w_{1} + \cdots + \lambda_{k}w_{k}> = \lambda_{1}<v, e_{1}> + \cdots + \lambda_{k}<v, w_{k}> = \lambda_{1} \cdot 0 + \cdots + \lambda_{k} \cdot 0 = 0$
				- per cui $v \in W^{\bot}$

## Domande

## Referenze
