---
tags:
  - category/lecture
  - status/finished
  - topic/calcolo-numerico
date: 23-09-2024 13:29:35
teacher: elena.loli@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- ripasso codifica floating-point
	- ricorda che di solito, se $x \notin \mathbb{F}$, si tronca il numero $x$ in $fl(x)$
	- se si fa l'arrotondamento tutti i numeri nel gap tra due numeri finiscono in quello successivo; con il troncamento è un po' diverso
	- nota bene: il gap aumenta man mano che mi allontano dallo zero, per via della caratteristica
		- i numeri più grandi sono più "discreti" tra di loro, ossia hanno un gap più grande
	- dobbiamo capire la precisione macchina e la sua proprietà
- errori e aritmetica floating-point
	- l'errore relativo di ogni operazione floating-point è sempre più piccolo della precisione di macchina
	- calcoli
		- $x \in \mathbb{R} \ \to \ fl(x) \in \mathbb{F}$
		- $y \in \mathbb{R} \ \to \ fl(y) \in \mathbb{F}$
		- allora $z = fl(x) \cdot fl(y)$ e $fl(z) \in \mathbb{F}$
		- l'ultimo passaggio è ciò che genera l'errore
	- esempio
		- $x = 2.15 \times 10^{12} = fl(x)$
		- $y = 1.25 \times 10^{-5} = fl(y)$
		- per fare un'operazione tra questi due numeri floating-point devo intanto portare le mantisse allo stesso esponente, quello più grande
		- $y = 0.0000000000000000125 \times 10^{12}$
		- ora faccio l'operazione: $z = x-y = 2.1499999999999999999 \times 10^{12} \notin \mathbb{F}$
		- allora faccio un troncamento fermandomi alle prime due cifre decimali: $fl(z) = 2.14 \times 10^{12}$
	- altro esempio
		- $\mathbb{F}(10, 5, -8, 8)$
		- $\text{eps} = \frac{1}{2} \cdot 10^{1-t} = \frac{1}{2} \cdot 10^{-4}$
		- $x = 1 \cdot 10^{0}$
		- $y = 2.345 \cdot 10^{-8}$
			- nota bene: $y < \text{eps}$
		- $fl(x) = 1 \cdot 10^{0} = x$
		- $fl(y) = 2.345 \cdot 10^{-8} = y$
		- $z = fl(x) + fl(y) = 1 \cdot 10^{0} + 2.345 \cdot 10^{-8} = (1 + 0.00000002345) \cdot 10^{0} = 1.00000002345 \cdot 10^{0}$
		- ma $z \notin \mathbb{F}$ (ha più di 5 cifre di mantissa), allora faccio troncamento $fl(z) = 1.00000 \cdot 10^{0} \in \mathbb{F}$ e $fl(z) = 1 = x$, per cui $1 + y = 1$, perché $y < eps$
		- di fatto $\text{eps}$ è il più piccolo numero che sommato a 1 fa un numero maggiore di 1 appartenente alla codifica $\mathbb{F}$
	- esempio di errore catastrofico --> calcolo di $e$ con limite $e = \lim_{n \to +\infty} \left(1 + \frac{1}{n}\right)^{n}$
		- l'errore diminuisce per poi, per $n$ troppo grandi, risalire
		- esempio, per $(1 + 10^{-16})^{10^{16}}$, si ha che $10^{-16}$ è più piccolo della precisione macchina, per cui si perde completamente e la parentesi diventa semplicemente $1$
		- l'errore provocato dalla propagazione delle approssimazioni/troncamenti delle operazioni floating-point viene chiamato **errore algoritmico**
- root finding by interval halving - **algoritmo di bisezione**, ossia calcolo degli zeri di una funzione/di una radice di un'equazione (non lineare)
	- forma di un'equazione generica: $f(x) = 0$
	- procedimenti:
		1. prima di tutto ci deve chiedere l'esistenza (e unicità) della soluzione
		2. poi si applica un algoritmo per calcolare la soluzione
	- per il primo punto: usiamo il [[Teorema degli zeri]], se $f: [a, b] \to \mathbb{R}$ continua in $[a, b]$, e $f(a) \cdot f(b) < 0$ allora $\exists x \in [a, b] : f(x) = 0$
	- per il secondo punto: usiamo la **bisezione**
		- in realtà ci sono in generale più algoritmi per uno stesso problema numerico, come il problema dell'[[Algoritmo di ordinamento|ordinamento di un array]] --> ciò che cambia è il [[Complessità computazionale|costo computazionale]]
			- in più tengono conto dell'accuratezza e quindi di quanto grande può essere l'errore --> questo perché lavoriamo con problemi di calcolo numerico, e quindi con numeri codificati
		- esempio:
			- vogliamo sapere quando $x = \cos(x)$, ossia trovare gli zeri di $f(x) = x - \cos(x)$
			- consideriamo come intervallo di studio $[a, b] = [-1, 1]$
		- l'idea dell'algoritmo di bisezione è quello di zoomare sulla funzione cambiando il valore dell'intervallo per mantenere la radice all'interno di esso
		- prendo il punto in mezzo all'intervallo $c$, e scelgo a seconda del suo valore $f(c)$ se assegnarlo al nuovo $a$ o al nuovo $b$ --> sfrutto sempre il teorema degli zeri
		- in particolare ogni "zoom" (iterazione):
			1. $c = \frac{a+b}{2}$
			2. $f(a) \cdot f(c) < 0 \implies [a, b] = [a, c]$ (o più semplicemente $b = c$)
			3. else $f(b) \cdot f(c) < 0 \implies [a, b] = [c, b]$ (o più semplicemente $a = c$)
		- la radice approssimata (il risultato dell'algoritmo) è l'ultimo valore di $c$
			- infatti si ha che $|c-x| < \frac{b-a}{2}$, ossia la distanza di $c$ dalla radice effettiva $x$ è minore della distanza dell'intervallo $[a, b]$
		- nota bene: grande somiglianza con la dimostrazione del [[Teorema degli zeri]]
		- codice dell'algoritmo in Python
	- sempre per il secondo punto, altro algoritmo: **iterazione di punto fisso**
		- l'algoritmo di bisezione è lento, questo meno
		- questo algoritmo è un algoritmo iterativo
			- nota bene: in teoria anche quello di bisezione, ma è un po' falso
		- assegnato un valore iniziale $x_{0}$, costruisco una successione di valori $x_{0}, x_{1}, x_{2}, \cdots, x_{k}, x_{k+1}, \cdots$ (potenzialmente infinita) che si ricavano l'uno dall'altro, $x_{k} = G(x_{k-1})$ dove $G$ è una funzione (o algoritmo), tale che $\lim_{k \to +\infty} x_{k} = x$ dove ($x$ è la radice della funzione $f$)
			- nota bene: la proprietà della successione è già verificata dalla teoria, formalmente già si dimostra che converge a $x$
			- nota bene: si tratta di un procedimento infinito, per cui dev'essere _troncato a un procedimento finito_ --> questo provoca un [[Errore di troncamento|errore di troncamento]][^1]
				- nasce quindi il problema degli algoritmi iterativi: **quando mi fermo**?
				- _trade-off tra accuratezza dell'algoritmo e tempo di calcolo_
				- per rispondere alla domanda ci sono dei **criteri di arresto**
				- di solito un algoritmo iterativo si implementa con un `while` che viene fermato da questi criteri
		- teoria dell'algoritmo
			- sposta il problema a trovare il punto fisso di una funzione $g$, relazionata a $f$ --> calcolare il punto fisso di $g$ serve a trovare la radice di $f$
				- ricorda che il punto fisso di una funzione è un punto $p$ t.c. $g(p) = p$
			- infatti $g(x) = x - \omega(x)f(x)$ dove $0 < \omega(x) < +\infty$
				- si capisce perché $f(x) = 0 \iff g(x) = x$, ossia $x - \omega(x)f(x) = x$
				- dimostriamo entrambe le implicazioni
					1. $\implies$: $f(\bar{x}) = 0 \implies \bar{x} - \omega(\bar{x})f(\bar{x}) = \bar{x} \implies g(\bar{x}) = \bar{x}$
					2. $\impliedby$: $\bar{x} - \omega(\bar{x})f(\bar{x}) = \bar{x} \implies -\omega(\bar{x})f(\bar{x}) = 0 \implies f(\bar{x}) = 0$ perché per ipotesi $0 < \omega(x) < +\infty$

## Domande

## Referenze
[^1]: non è la tecnica di troncamento di rappresentazione di un numero (che fa invece parte degli [[Errore di arrotondamento|errori di arrotondamento]])