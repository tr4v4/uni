---
tags:
  - category/lecture
  - status/finished
  - topic/introduzione-all-apprendimento-automatico
date: 10-10-2025 13:10:23
teacher: andrea.asperti@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- pre-appello a Dicembre
- per il naive bayes
	- avevamo detto che usavamo la maximum likelihood per stimare le probabilità, era il modo semplice
	- possiamo dimostrare della sua validità con un esempio di distribuzione binomiale
		- poi si generaliiza nel caso multivalore
	- applicazione di naive bayes: classificazione di documenti
		- usiamo tecnica "bag of words"
		- vediamo ogni documento come una sequenza di parole
		- ci aspettiamo che le varie classi usino un dizionario leggermente diverso
		- gli eventi elementari sono le parole che occorono in posizione $i$ nel documenti, su ciascuna parola definiamo una variabile aleatoria $X_{i}$ che assume tanti possibili valori quante sono le parole nel vocabolario
		- $\theta_{i, word, l} = \mathbb{P}(X_{i} = word | Y = l)$
		- assumiamo che i documenti siano indipendenti tra di loro --> la probabilità che una parola capiti in una posizione o in un'altra, è indipendente; e la probabilità che una parola compaia nello stesso documenti in cui c'è un'altra parola, è indipendente
		- di conseguenza $\theta_{i, word, l} = \theta_{j, word, l} = \theta_{word, l}$
		- usiamo MLE
			- dobbiamo stimare la probabilità a priori che il documento che stiamo analizzando appartenga a una certa categoria (possiamo usare il training set nel modo più naive)
			- e poi dobbiamo stimare, data la categoria del documento, la probabilità che ogni parola appartiene a tale categoria
		- di solito si passa sempre alla _log likelihood_
			- si cerca la categoria che massimizza il logaritmo --> semplicemente semplifica i calcoli, perché da un prodotto otteniamo una sommatoria
			- abbiamo un _dot product_
				- e cerchiamo il massimo dot product, ossia la correlazione
					- nel caso geometrico è quanto sono "simili" i vettori in base all'angolo
					- nel caso analitico è il prodotto scalare
			- il motivo per cui $n_{j}$ non appare come logaritmo è per via della cross-entropy, infatti notare la somiglianza con la definizione di entropia
	- natura lineare - caso booleano
		- lineare - associamo ad ogni feature un peso moltiplicativo diverso, ne facciamo una [[Combinazione lineare]] e a seconda del threshold facciamo la classificazione
		- naive bayes è lineare - lo dimostriamo
		- tecniche di classificazione lineari
			- per cui ogni caratteristica del dato è valutata indipendentemente dalle altre
- naive bayes gaussiano
	- cosa facciamo quando le features $X_{i}$ sono continue?
	- l'approccio tradizionale consiste nel supporre $\mathbb{P}(X_{i}|Y)$ abbia una [[Distribuzione normale]]

## Domande

## Referenze
