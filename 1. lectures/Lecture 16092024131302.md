---
tags:
  - category/lecture
  - status/finished
  - topic/calcolo-numerico
date: 16-09-2024 13:13:02
teacher: elena.loli@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- programma del corso:
	- numeri finiti ed errori
	- algebra lineare numerica
	- approssimazione di dati
	- ottimizzazione
	- risoluzione numerica di problemi inversi mal posti
	- esercitazioni in Python
	- applicazioni con esercitazioni in Python
- obiettivo:
	- usare modelli matematici per risolvere problemi con il calcolatore --> la parte finale è scrivere un programma
	- si lavora con i numeri
- bibliografia
	- notebook sugli argomenti, sia su teoria che su codice (tutto in Python)
		- https://pythonnumericalmethods.studentorg.berkeley.edu/notebooks/Index.html
		- https://lemesurierb.people.charleston.edu/elementary-numerical-analysis-python/preface.html
		- https://gustavdelius.github.io/NumericalAnalysis2024/
- esame
	- un unico esame per entrambi i moduli
	- 2 parti:
		- scritta con domande a risposta multipla, su programma di lezione e laboratorio (max 20), 45 minuti di tempo
		- breve orale di discussione dei tre progetti assegnati (max 11)
- argomento: **numeri finiti** (2 risorse su virtuale)
	- in base 10: $147.3 = 1 \cdot 10^{2} + 4 \cdot 10^{1} + 7 \cdot 10^{0} + 3 \cdot 10^{-1} = 147.3_{10}$
		- rappresentiamo un numero in una base
	- in base 3: $121_{3} = 1 \cdot 3^{2} + 2 \cdot 3^{1} + 1 \cdot 3^{0} = 9 + 6 + 1 = 16_{10}$
	- in base 2 (su computer): $11_{10} = 8 + 2 + 1 = 1 \cdot 2^{3} + 0 \cdot 2^{2} + 1 \cdot 2^{1} + 1 \cdot 2^{0} = 1011_{2}$
		- le cifre 1011 sono ottenute attraverso il _metodo delle divisioni successive_
			- nota bene: da base $n$ a base 10 è facile, per il contrario bisogna usare questo metodo
	- conversione numero decimale (binario)
		- $(0.1)_{10}$ a base 2: già fatta --> $= (0.000110011...)_{2}$
			- questo è il _metodo delle moltiplicazioni successive_
			- nota bene: il numero in base binaria è periodico, mentre in base decimale no, e questo è il problema già affrontato in architettura
				- certi numeri reali devono essere approssimati in binario su un computer
				- per ovviare a questo problema si usa la [[Codifica floating-point|codifica floating-point]]
					- in breve divide il numero in 3 parti:
						- segno $s$
						- caratteristica $e$
						- mantissa $f$
					- esempio Python usa IEEE754, ossia 64 bit totali divisi in: 1 per il segno; 11 per la caratteristica; 52 per la mantissa
					- codice Python per vedere ciò: `import sys` e `sys.float_info`
						- si scopre di interessante che con questa codifica il numero di cifre significative dopo la virgola è 15 (è inutile provare a ottenere 17 cifre dopo la virgola)
					- con questa codifica ogni numero float può essere rappresentato come $$n = (-1)^{s} \cdot 2^{e-1023} \cdot (1 + f)$$
						- attenzione: quel $2^{e-1023}$ è perché $e \in [0, 2048[ \subseteq \mathbb{N}$, per cui per esponenti negativi usiamo valori di $e$ tra $0$ e $1022$
						- attenzione: $f \in [0, 1[ \subseteq \mathbb{R}$, per cui $(1+f) \in [1, 2[$
					- _generalizzazione matematica della codifica floating-point_:
						- $\mathbb{F} (\beta, t, +L, u)$ ossia base, numero di cifre in mantissa e range dell'esponente ($L, u$)
							- in particolare $$\mathbb{F}(\beta, t, +L, u) = \{\{\varnothing\} \cup n = \pm \beta^{p} \cdot d_{0}, d_{1}, \cdots, d_{t} | -L \leq p \leq u\}$$
						- esempio la IEEE745 è $\beta = 2$, $t = 52$, $L = -1023$ e $u = 1025$ --> $\mathbb{F} (2, 52, 1023, 1024)$
						- ora, preso un $x$, ci sono due possibilità:
							- $x \in \mathbb{F}$
							- $x \notin \mathbb{F}$
							- e nel secondo caso non rimane che approssimare $x$
							- questo numero si indica con $fl(x)$, per cui:
								- $x \in \mathbb{F} \implies fl(x) = x$
								- $x \notin \mathbb{F} \implies fl(x) \neq x$
							- _per approssimare tronco la mantissa al numero di cifre $t$_
						- caratteristiche della codifica:
							- i numeri sono discreti
							- intorno allo 0 c'è il vuoto
							- esiste un minimo e un massimo
						- esempio $\mathbb{F}(10, 4, 3, 3)$
							- il max è $9.9999 \cdot 10^{3} = 9999.9$
							- il min è $1.0000 \cdot 10^{-3}$
						- nota bene:
							- la caratteristica influenza minimo e massimo (perché più è alto l'esponente più il numero può essere grande o piccolo)
							- la mantissa invece influenza la distanza tra i numeri discreti (perché più è il numero di cifre più la distanza è piccola tra i numeri)
						- utilizzeremo tantissimo _numpy_, e per esempio la funzione `np.spacing(1e9)` --> questo è il _gap_ tra i numeri in floating-point
						- in Python la codifica IEEE745 include `inf`, `-inf` e `NaN` (Not a Number)
						- sempre in Python si ha che `max + 2 == max` dà `True`
							- questo perché 2 è piccolo
							- se invece si somma `max + max` questo dà `inf`
	- ora passiamo agli **errori**, come si gestiscono
		- in particolare degli _errori di arrotondamento_, e quindi degli _errori di troncamento_ (ossia che si interrompe un processo)
		- errori di arrotondamento
			- quando devo approssimare un numero perché ha infinite cifre (periodico o irrazionale)
			- in Python si ha che `4.9 - 4.845 == 0.055` dà `False`!
				- perché in base 2 non ho la rappresentazione esatta degli operandi
				- questo comporta che in programmazione di solito non si fa MAI `if a == b`, perché se si lavora con i numeri ci sono questi piccoli errori di arrotondamento; invece si scrive `if |a-b| < tol` dove `tol` è un valore di tolleranza (tipo $10^{-10}$)
			- formalmente si scrive:
				- _errore assoluto di arrotondamento_ --> $|fl(x) - x| < \beta^{1-t}$
					- per esempio se prendiamo $\mathbb{F}(10, 4, 3, 3)$ allora il più grande errore è $10^{1-4} = 10^{-3}$
					- invece per $\mathbb{F}(2, 52, 1023, 1025)$ si ha $\beta^{1-t} = 2^{1-52} = 2^{-51}$
					- nota bene: ovviamente questo valore dipende da $t$, ovvero dalle cifre della mantissa
				- _errore relativo di arrotondamento_ --> $\frac{|fl(x)-x|}{|x|} < \frac{1}{2}\beta^{1-t}$
					- si ha che $eps=\frac{1}{2}\beta^{1-t}$ è detta _precisione macchina_, ossia il più piccolo numero macchina ($\in \mathbb{F}$) positivo tale che $fl(1 + eps) > 1$
	- **aritmetica floating-point**
		- ogni operazione in floating-point, di due numeri in $\mathbb{F}$, anche il loro risultato deve tornare in $\mathbb{F}$[^1]
		- allora si ha che, indicando $\cdot: \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ l'operazione aritmetica reale, e $\odot: \mathbb{F} \times \mathbb{F} \to \mathbb{F}$ come l'operazione floating-point, si ha che $$x \odot y = fl(x \cdot y)$$
		- ogni operazione di questo tipo provoca in generale un errore, detto errore di arrotondamento (molto piccolo): $$\left|\frac{(x \odot y) - (x \cdot y)}{x \cdot y}\right| < eps$$

## Domande

## Referenze
[^1]: chiusura!