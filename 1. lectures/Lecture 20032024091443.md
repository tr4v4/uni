---
tags:
  - category/lecture
  - status/finished
  - topic/algebra-e-geometria
date: 20-03-2024 09:14:43
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- **teorema della dimensione**
	- sia $f: V \to W$ lineare allora $$\dim(V) = \dim(\ker(f)) + \dim(\Im(f))$$
	- utilità
		- per calcolare il nucleo sappiamo di dover risolvere $A\underline{x} = \underline{0}$
		- per calcolare l'immagine dobbiamo mettere $A^{T}$ in Gauss
		- ma se vogliamo sapere solo le dimensioni di nucleo e immagine (per sapere per esempio se $f$ è iniettiva), possiamo calcolare solo una delle due dimensioni e ricavare l'altra
	- dimostrazione (bisogna ricordarsi l'idea):
		- sia $\dim(V) = n$ e $\dim(\ker(f)) = r$, allora mostriamo che $\dim(\Im(f)) = n - r$
		- per ipotesi sia $\{v_{1}, \cdots, v_{r}\}$ una base di $\ker(f)$, per cui $v_{1}, \cdots, v_{r}$ sono linearmente indipendenti, che per il teorema del completamento possiamo completare a una base $\beta = \{v_{1}, \cdots, v_{r}, v_{r+1}, \cdots, v_{n}\}$ di $V$ (t.c. $v_{r+1}, \cdots, v_{n}$ sono $n-r$ vettori)
		- mostriamo che $\{f(v_{r+1}), \cdots, f(v_{n})\}$ è base di $\Im(f)$, ovvero che $\dim(\Im(f)) = n-r$
			- devono generare: sfruttiamo teorema che $\Im(f) = \langle f(v_{1}), \cdots, f(v_{r}), f(v_{r+1}), \cdots, f(v_{n}) \rangle$, e sappiamo che $f(v_{1})=\cdots=f(v_{r}) = \underline{0}$ perché stanno nel nucleo, per cui sopravvivono solo $f(v_{r+1}), \cdots, f(v_{n})$ che generano $\Im(f)$
			- devono essere linearmente indipendenti: dobbiamo usare la definizione, per cui sia $\lambda_{r+1}f(v_{r+1}) + \cdots + \lambda_{n}f(v_{n}) = \underline{0} \iff \lambda_{r+1} = \cdots = \lambda_{n} = 0$
			- per la linearità di $f$ otteniamo $f(\lambda_{r+1}v_{r+1} + \cdots + \lambda_{n}v_{n}) = \underline{0}$, per cui $v = \lambda_{r+1}v_{r+1} + \cdots + \lambda_{n}v_{n} \in \ker(f)$, e sappiamo che $\ker(f) = \langle v_{1}, \cdots, v_{r} \rangle$
			- per cui $v = \lambda_{1}v_{1} + \cdots + \lambda_{r}v_{r}$, allora otteniamo l'uguaglianza $\lambda_{1}v_{1} + \cdots + \lambda_{r}v_{r} = \lambda_{r+1}v_{r+1} + \cdots + \lambda_{n}v_{n}$, da cui $\lambda_{1}v_{1} + \cdots + \lambda_{r}v_{r} - \lambda_{r+1}v_{r+1} - \cdots - \lambda_{n}v_{n} = 0$ e appartenendo a $\beta$ i vettori sono linearmente indipendenti, per cui per fare 0 bisogna per forza avere $\lambda_{1} = \cdots = \lambda_{r} = \lambda_{r+1} = \cdots = \lambda_{n} = 0$, in particolare $\lambda_{r+1} = \cdots = \lambda_{n} = 0$
			- allora si ha che i vettori $f(v_{r+1}), \cdots, f(v_{n})$ sono linearmente indipendenti
	- esempio
		- non esistono applicazioni lineari iniettive $f: \mathbb{R}^{5} \to \mathbb{R}^{3}$, dimostrazione
			- per assurdo se $f$ esiste allora $\dim(\mathbb{R}^{5}) = \dim(\ker(f)) + \dim(\Im(f))$ per cui $\dim(\mathbb{R}^{5}) = 5$, $\dim(\Im(f)) \leq 3$ ma essendo $f$ iniettiva $\dim(\ker(f)) = 0$, allora $5 = 0 + 3$ che è un assurdo
			- il nucleo deve avere per forza dimensione almeno 2
		- non esistono applicazioni lineari suriettiva $f: \mathbb{R}^{4} \to \mathbb{R}^{7}$, dimostrazione
			- perché presa un $f$ lineare allora $\dim(\mathbb{R}^{4}) = \dim(\ker(f)) + \dim(\Im(f))$ e ho $\dim(\mathbb{R}^{4}) = 4$, $\dim(\Im(f)) = 7$ perciò $4 = x + 7$ richiede $\dim(\ker(f)) = -3$
			- in generale proposizione: siano $V, W$ spazi vettoriali, se $\dim(V) \leq \dim(W)$ allora non esistono applicazioni lineari suriettive $f: V \to W$
				- dimostrazione: $\dim(\Im(f)) = \dim(V) - \dim(\ker(f)) \leq \dim(V) < \dim(W)$
			- altra proposizione: se $\dim(V) > \dim(W)$ allora non esistono applicazioni lineari iniettive $f: V \to W$
				- dimostrazione: $\dim(\ker(f)) = \dim(V) - \dim(\Im(f)) \geq \dim(V) - \dim(W) > 0$
		- esercizio
			- attenzione: gli altri anni c'era un esercizio che diceva "stabilire se questo è un sottospazio" (abbastanza difficile); ma quest'anno non ce lo mette questo esercizio
			- sia $f_{k}: \mathbb{R}^{4} \to \mathbb{R}^{3}$ lineare (con $k$ significa che dipende da un parametro) definita da $f_{k}(e_{1}) = e_{1} + 2e_{2} + ke_{3}$, $f_{k}(e_{2}) = ke_{2}+ke_{3}$, $f_{k}(e_{3}) = ke_{1}+ke_{2}+6e_{3}$, $f_{k}(e_{4}) = ke_{1}+(6-k)e_{3}$
				- ricordiamo che se so come $f_{k}$ funziona su una base so come funziona in generale, per cui scriviamo la matrice associata $A$
				- $$A = \begin{pmatrix} 1 & 0 & k & k \\ 2 & k & k & 0 \\ k & k & 6 & 6-k \end{pmatrix}$$
				- l'esercizio ci chiede
					1. per quali $k$ abbiamo $f_{k}$ iniettiva e per quali è suriettiva?
					2. scelto $k = a$ t.c. $f_{a}$ non è suriettiva si determini $v \in \mathbb{R}^{3}$ t.c. $v \notin \Im(f)$
					3. discutere la dimensione di $\ker(f_{k})$ con $k \in \mathbb{R}$
				- svolgimento
					1. $f_{k}$ non è mai iniettiva perché $\dim(\mathbb{R}^{4}) > \dim(\mathbb{R}^{3})$, quindi per il teorema della dimensione; per la suriettività sfruttiamo il teorema $\Im(f) = \langle f(e_{1}), \cdots, f(e_{4}) \rangle$ ovvero l'immagine è generato dalle colonne di $A$, allora usiamo Gauss in modo diretto ma su $A$ trasposta! ovvero su $A^{T}$; questo per scoprire per quali $k$ è suriettiva $f_{k}$; otteniamo che $\Im(f) = \langle (1, 2, k), (0, k, k), (0, 0, k^{2}-k-6) \rangle$, e quindi se $k \neq 0 \land k^{2}-k-6 \neq 0 \implies \{w_{1}, w_{2}, w_{3}\}$ base di $\Im(f)$, per cui $\dim(\Im(f)) = 3 = \dim(\mathbb{R}^{3}) \implies f$ suriettiva; allora $f$ è suriettiva $\iff k \neq 0 \land k \neq 3 \land k \neq -2$; per $k = 0$ invece otteniamo $\dim(\Im(f)) = 2 < 3 = \dim(\mathbb{R}^{3})$ per cui $f_{0}$ non è suriettiva
					2. scelgo $k = 0$ e trovo un vettore $v \in \Im(f)$; se $a = 0$ abbiamo $\Im(f_{0}) = \langle (1, 2, 0), (0, 0, -6) \rangle$, allora scelgo $(0, 1, 3) \notin \Im(f)$
					3. usiamo il teorema della dimensione
	- corollario
		- proposizione: siano $V, W$ spazi vettoriali t.c. $\dim(V) = \dim(W)$ e sia $f: V \to W$ lineare allora $f$ è iniettiva $\iff$ $f$ è suriettiva
			- questa è una conseguenza del teorema della dimensione, infatti $\dim(V) = \dim(\ker(f)) + \dim(\Im(f))$, e se $f$ è iniettiva allora $\dim(\ker(f)) = 0$, perciò $\dim(V) = \dim(\Im(f)) = \dim(W) \implies$ $f$ suriettiva;
		- proposizione: sia $A \in M_{m \times n} (\mathbb{R})$, allora $\dim(\text{colonne di A}) = \dim(\text{righe di A})$
			- presa ad esempio $A = \begin{pmatrix} 1 & 3 & 2 & 1 & 1 \\ 2 & 6 & 5 & 3 & 1 \\ 0 & 0 & 4 & 4 & -4 \end{pmatrix}$, abbiamo che $\dim(\langle r_{1}, r_{2}, r_{3} \rangle) = 2$, per cui $\langle r_{1}, r_{2}, r_{3} \rangle \leq \mathbb{R}^{5}$; il sottospazio generato dalle colonne ha $\dim(\langle c_{1}, c_{2}, c_{3}, c_{4}, c_{5} \rangle) = 2$ nonostante $\langle c_{1}, c_{2}, c_{3}, c_{4}, c_{5} \rangle \leq \mathbb{R}^{3}$
			- questo è vero per il teorema della dimensione
			- dimostrazione: abbiamo $A$, consideriamo ora l'applicazione lineare associata $L_{A}: \mathbb{R}^{n} \to \mathbb{R}^{m}$ t.c. $\underline{x} \to A\underline{x}$; consideriamo allora il kernel $\ker(L_{A}) = \{x \in \mathbb{R}^{n} | A\underline{x} = \underline{0}\}$, quindi l'insieme delle soluzioni che dipendono da $n-r$ parametri, quindi $\dim(\ker(L_{A})) = n-r$, allora $\dim(\langle r_{1}, \cdots, r_{m} \rangle) = r$
				- $\Im(L_{A}) = \langle L_{A}(e_{1}), \cdots, L_{A}(e_{n}) \rangle$ = colonne di $A$, e $\langle c_{1}, \cdots, c_{n} \rangle = V$
				- usiamo Gauss in modo diretto su $A^{T}$ in modo da avere le colonne in riga --> otteniamo $t$ pivot, allora $\dim(\Im(L_{A})) = t$
				- il teorema della dimensione ci dice che $\dim(V) = \dim(\ker(L_{A})) + \dim(\Im(L_{A}))$, allora otteniamo $$n = n-r + t \implies r = t$$

## Domande
- counter lavagne = 27

## Referenze
