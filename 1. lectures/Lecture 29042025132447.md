---
tags:
  - category/lecture
  - status/pending
  - topic/calcolo-delle-probabilità-e-statistica
date: 29-04-2025 13:24:47
teacher: elena.bandini7@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- vettori aleatori
	- consideriamo $h: \mathbb{R}^{2} \to \mathbb{R}$ e $(X, Y)$ vettore aleatorio discreto, allora $h(X, Y)$ è una variabile aleatoria e
		- $\mathbb{E}[h(X, Y)] = \sum\limits_{i,j} h(x_{i}, y_{j}) p_{(X, Y)}(x_{i}, y_{j})$
		- $Var(h(X, Y)) = \mathbb{E}[h^{2}(X, Y)] - (\mathbb{E}[h(X, Y)])^{2}$
	- teorema, linearità del valore atteso: $(X, Y)$ vettore aleatorio discreto, $a, b \in \mathbb{R}$, allora $\mathbb{E}[aX + bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]$
		- dimostrazione: consideriamo $h(x, y) = ax + by$, allora $\mathbb{E}[aX + bY] = \mathbb{E}[h(x, y)] = a \sum\limits_{i}x_{i}\sum\limits_{j}p_{(X, Y)}(x_{i}, y_{j}) + b \sum\limits_{j}y_{j}\sum\limits_{i}p_{(X, Y)}(x_{i}, y_{i})$
			- ma questo è $a \sum\limits_{i}x_{i}p_{X}(x_{i}) + b\sum\limits_{j}y_{j}p_{Y}(y_{j}) = a\mathbb{E}[X] + b\mathbb{E}[Y]$
		- corollario: $(X, Y)$ vettore aleatorio discreto e $X, Y$ indipendenti, allora $\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]$
			- dimostrazione
	- **indici dei vettori**
		- la media diventerà il vettore delle medie
		- l'oggetto relativo al vettore $(X, Y)$ sarà una matrice, che introduce la covarianza
		- quindi
			- vettore delle medie $\in \mathbb{R}^{2}$ (se il vettore è bidimensionale)
			- matrice delle covarianze $\in \mathbb{R}^{2 \times 2}$
				- $$\begin{pmatrix} Var(X) & Cov(X, Y) \\ Cov(Y, X) & Var(Y) \end{pmatrix}$$
				- osservazioni:
					- $Var(X) = Cov(X, X)$
					- $Cov(X, Y) = Cov(Y, X)$, matrice simmetrica
				- definizione: $(X, Y)$ vettore aleatorio, allora $Cov(X, Y) = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]$
				- se $Cov(X, Y) = 0$ si dice che $X$ e $Y$ sono incorrelate
				- lemma: $(X, Y)$ vettore aleatorio (discreto), allora $Cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$
					- dimostrazione
				- teorema: $X, Y$ indipendenti $\implies$ $Cov(X, Y) = 0$
					- notare il verso dell'implicazione --> se la covarianza è 0 significa che non c'è una dipendenza _lineare_ tra $X$ e $Y$; se invece $X, Y$ sono indipendenti, non c'è una dipendenza _funzionale_ tra $X$ e $Y$
				- definizione, coefficiente di correlazione: $\rho_{X, Y} = \frac{Cov(X, Y)}{\sqrt{Var(X)Var(Y)}} \in [-1, 1]$ è detto coefficiente di correlazione tra $X$ e $Y$
					- $\rho_{X, Y} = \pm 1 \iff Y = aX + b$ per qualche $a, b \in \mathbb{R}$
					- $\rho_{X, Y} \in (0, 1) \iff \text{approssimativamente dipendenza lineare}$ --> caso della regressione lineare!
					- $\rho_{X, Y} = 0 \iff \text{non ho alcuna dipendenza lineare}$
					- quindi, l'indipendenza è un qualcosa di molto più forte, perché elimina una qualunque dipendenza funzionale, per cui anche quella lineare --> se invece sappiamo che la covarianza è 0, possiamo solo escludere la dipendenza lineare!
					- 

## Domande

## Referenze
