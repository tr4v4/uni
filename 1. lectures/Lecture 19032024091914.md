---
tags:
  - category/lecture
  - status/pending
  - topic/algebra-e-geometria
date: 19-03-2024 09:19:14
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- [[Applicazione lineare|applicazioni lineari]]
	- ripasso delle proprietà
	- ripasso su [[Nucleo]] e [[Immagine di funzione]]
	- ripasso su suriettività e iniettività di $f$ lineare
	- ripasso su [[Teorema di esistenza e unicità di un'applicazione lineare]]
	- di nuovo $f = L_{A}$ se $f$ lineare
		- come faccio a capire $A$? le sue colonne sono le proiezioni di $f$ per le basi canoniche $e_{1}, \cdots, e_{n}$
			- _$A$ è la matrice che ha $f(e_{i})$ nella $i$-esima colonna_
			- analogamente $L_{A}(e_{i}) = i$-esima colonna di $A$
	- abbiamo allora 3 modi di dare un'applicazione lineare
		1. assegnare $f(e_{1}), \cdots, f(e_{n})$ (_formula chiave_)
		2. dare la legge $f(x_{1}, \cdots, x_{n}) = (\cdots, \cdots, \cdots)$
		3. dare la matrice $A$
		- nota bene: all'esame ce ne dà uno e noi dobbiamo ricavare gli altri
	- calcoliamo $\ker(f)$ sull'esempio della volta scorsa:
		- $\ker(f) = \{\underline{x} \in \mathbb{R}^{5} | f(\underline{x}) = \underline{0}\} = \{\underline{x} \in \mathbb{R}^{5} | A\underline{x} = \underline{0}\}$
		- quindi il nucleo è l'insieme delle soluzioni del sistema lineare omogeneo associato ad $A$
		- per questo motivo se abbiamo $f$ ci conviene calcolare $A$, perché da essa ricaviamo subito il nucleo
		- il sistema è $$\begin{pmatrix} 1 & 3 & 2 & 1 & 1 & 0 \\ 2 & 6 & 5 & 3 & 1 & 0 \\ 0 & 0 & 4 & 4 &-4 & 0 \end{pmatrix}$$
			- da cui con Gauss ricaviamo $$\begin{pmatrix} 1 & 3 & 2 & 1 & 1 & 0 \\ 0 & 0 & 1 & 1 & -1 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 \end{pmatrix}$$
			- per cui ci sono infinite soluzioni che dipendono da 3 parametri
				- ricaviamo $x_{1}, x_{3}$ (le incognite dei pivot, per semplicità)
			- otteniamo quindi $$\begin{cases} x_{1}+3x_{2}+2x_{3}+x_{4}+x_{5} = 0 \\ x_{3}+x_{4}-x_{5} = 0 \end{cases} \implies S = \{(-3x_{2}+x_{4}-3x_{5}, x_{2}, -x_{4}+x_{5}, x_{4}, x_{5}) | x_{2}, x_{4}, x_{5} \in \mathbb{R}\}$$
			- per cui raccogliamo e otteniamo il sottospazio generatore, infatti vogliamo una base di $\ker(f)$
			- $$\ker(f) = \{x_{2}(-3, 1, 0, 0, 0) + x_{4}(1, 0, -1, 1, 0) + x_{5}(-3, 0, 1, 0, 1) | x_{2}, x_{4}, x_{5} \in \mathbb{R}\}$$
			- per cui abbiamo $$\ker(f) = \langle (-3, 1, 0, 0, 0), (1, 0, -1, 1, 0), (-3, 0, 1, 0, 1) \rangle = \langle v_{1}, v_{2}, v_{3} \rangle$$
			- nota bene: $v_{1}, v_{2}, v_{3}$ sono una base di $\ker(f)$, perché sono anche linearmente indipendenti --> il nucleo ha dimensione $\dim(\ker(f)) = 3$
				- si ottengono in modo più facile di così, infatti
					- $v_{1}$ si ottiene con $x_{2}=1, x_{4}=0, x_{5}=0$
					- $v_{2}$ si ottiene con $x_{2}=0, x_{4}=1, x_{5}=0$
					- $v_{3}$ si ottiene con $x_{2}=0, x_{4}=0, x_{5}=1$
				- in generale diciamo che le soluzioni di $A\underline{x} = \underline{0}$ dipendono da $k = n - r$ parametri; il nucleo ha dimensione $k$, e una base $v_{1}, \cdots, v_{k}$ di $\ker(f)$ si ottiene nel modo seguente
					- $v_{i}$ è il vettore con l'$i$-esimo parametro $= 1$ e gli altri $= 0$
				- in ogni caso $v_{1}, v_{2}, v_{3}$ sono sempre base di $\ker(f)$ per un teorema che non dimostriamo
			- nota: abbiamo gratis che l'insieme delle soluzioni di un sistema lineare omogeneo è sempre un sottospazio di $\mathbb{R}^{n}$, perché $\ker(L_{A})$
	- calcoliamo ora la base di $\Im(f)$, quindi l'immagine di un'applicazione lineare
		- proposizione: $f: V \to W$ lineare e $\beta = \{v_{1}, \cdots, v_{n}\}$ base di $V$, allora $\Im(f) = \langle f(v_{1}), \cdots, f(v_{n}) \rangle$
			- dimostrazione:
				- riscriviamo definizione di immagine $\Im(f) = \{f(v) | v \in V\}$
				- riscriviamo definizione di sottospazio generato $\langle f(v_{1}), \cdots, f(v_{n}) \rangle = \{\lambda_{1}f(v_{1}) + \cdots + \lambda_{n}f(v_{n}) | \lambda_{1}, \cdots, \lambda_{n} \in \mathbb{R}\}$
				- mostriamo la doppia inclusione, perciò $\Im(f) \subseteq \langle f(v_{1}), \cdots, f(v_{n}) \rangle$
					- prendiamo $w \in \Im(f)$ t.c. quindi $w = f(v)$ per un $v \in V$
					- sappiamo $v_{1}, \cdots, v_{n}$ generano $V$, per cui $\exists \lambda_{1}, \cdots, \lambda_{n} \in \mathbb{R} | v = \lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n}$
					- allora $w = f(v) = f(\lambda_{1}v_{1} + \cdots + \lambda_{n}v_{n})$, per cui per la linearità di $f$ possiamo scrivere $w = \lambda_{1}f(v_{1}) + \cdots + \lambda_{n}f(v_{n}) \in \langle f(v_{1}), \cdots, f(v_{n}) \rangle$
				- mostriamo allora $\langle f(v_{1}), \cdots, f(v_{n}) \rangle \subseteq \Im(f)$
					- si dimostra facilmente tornando indietro dalla prima inclusione
					- oppure si usa una proprietà: osserviamo $f(v_{1}), \cdots, f(v_{n}) \in \Im(f)$, e $\Im(f) \leq V$, allora per [[Sottospazio generato#$Z leq V v_{1}, cdots, v_{n} in Z implies langle v_{1}, cdots, v_{n} rangle subseteq Z$|questa proposizione]] anche $\langle f(v_{1}), \cdots, f(v_{n}) \rangle \subseteq \Im(f)$
				- per la doppia inclusione vale l'uguaglianza
	- torna sull'applicazione lineare di prima con la stessa $A = \begin{pmatrix} 1 & 3 & 2 & 1 & 1 \\ 2 & 6 & 5 & 3 & 1 \\ 0 & 0 & 4 & 4 & -4 \end{pmatrix}$
		- con $L_{A}: \mathbb{R}^{5} \to \mathbb{R}^{3}$
		- vogliamo trovare una base di $\Im(L_{A})$, usando il teorema sopra
			- allora prendiamo la base canonica $\beta = \{e_{1}, \cdots, e_{5}\}$ di $\mathbb{R}^{5}$
			- allora si dice che $\Im(L_{A}) = \langle L_{A}(e_{1}), \cdots, L_{A}(e_{5}) \rangle = \langle (1, 2, 0), (3, 6, 0), (2, 5, 4), (1, 3, 4), (1, 1, -4) \rangle$
				- sappiamo ovviamente che questa non è base dell'immagine perché l'immagine vive in $\mathbb{R}^{3}$, che ha dimensione 3, per cui per il teorema del completamento 5 vettori non possono essere linearmente indipendenti
				- allora usiamo Gauss in modo diretto per trovare una base del sottospazio generato, da cui ricaviamo che $\dim(\Im(L_{A})) = 2$, e una base è $\{(1, 2, 0), (0, 1, 4)\}$
		- piccolo spoiler del teorema della dimensione

## Domande
- counter lavagne = 25

## Referenze
