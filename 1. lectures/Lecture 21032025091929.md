---
tags:
  - category/lecture
  - status/pending
  - topic/sistemi-operativi
date: 21-03-2025 09:19:29
teacher: renzo.davoli@unibo.it
mod: 2
---
# Lezione
---
## Concetti
- **gestione della memoria**
	- **paginazione** - ripasso
		- in realtà non elimina proprio la frammentazione esterna... riflessione
	- **segmentazione**
		- è un altro concetto, per localizzare la memoria
		- il processo ha bisogno della memoria per diversi scopi: per metterci codice, dati, stack; alcuni processi potrebbero anche volere aree di memoria condivise con altri processi
		- l'idea è di separare logicamente le aree di memoria a seconda del loro scopo --> aree di memoria con scopo diversi possono avere proprietà diverse
			- esempio: area `TEXT`, dove si mette il codice del programmi in esecuzione, dovrebbe essere di sola lettura --> di solito i programmi non si modificano mentre si eseguono; area `DATA` condivisa, magari; area `STACK` assolutamente non condivisibile
		- lo spazio di indirizzamento logico viene diviso in aree omogenee: un segmento è un'area di memoria logicamente continua, con elementi affini tra di loro
		- quindi, i segmenti più che un indirizzo hanno un nome
		- la paginazione è un'attività automatica, invece la suddivisione del programma in segmenti spetta al programmatore, o meglio, al compilatore!
		- i segmenti hanno dimensione variabile, con una dimensione tipicamente dai 64KB ai molti MB:w
		- la segmentazione quindi consente la condivisione di codice e dati
		- possiamo unire paginazione e segmentazione!
			- ogni segmento viene suddiviso in pagine che vengono allocate in frame liberi della memoria (non necessariamente contigui)
			- da un punto di vista hardware è necessario che la MMU supporti sia la segmentazione che la paginazione
			- implementazione:
				- si usano i primi tot. bit per il segmento, poi i bit centrali per la pagina, e infine gli ultimi per l'offset all'interno della pagina
	- **memoria virtuale**
		- è la tecnica che permette l'esecuzione di processi che non sono completamente in memoria
		- primite semplici, `LOAD` e `STORE`
		- consente di usare la memoria primaria + un pezzo di memoria secondaria, facendo apparire ai processi una memoria utilizzabile più ampia
		- funzione perché: se analizziamo il pattern di accesso in memoria dei processi, non c'è una distribuzione omogenea degli accessi --> tipicamente si accede a indirizzi contigui
			- per esempio, la routine di gestione degli errori potrebbe non essere mai chiamata in un programma --> non ha senso caricarla in memoria primaria
			- oppure nei compilatori le fasi usano strutture dati differenti
		- implementazione
			- ogni processo ha uno spazio di indirizzamento virtuale che può essere più grande di quello fisico
			- gli indirizzi virtuali possono essere mappati su indirizzi fisici della memoria principale, o su indirizzi fisici della memoria secondaria
			- in caso di accesso a indirizzi virtuali mappati in memoria secondaria si ha che:
				1. si arresta il processo
				2. i dati sono trasferiti nella memoria principale
				3. se la memoria è piena si sposta in memoria secondaria i dati contenuti in memoria principali considerati meno utili
				4. si riprende il processo
			- ma quindi: nel caso di memoria primaria piena, quale allocazione di memoria spostiamo in memoria secondaria? su quale base di valutazione?
				- l'idea è di ripensare la paginazione, facendo la memoria virtuale usando una paginazione particolare: _demand paging_
				- ricorda: la paginazione nasce come metodo per limitare la frammentazione, poi può essere usata per fare memoria virtuale comodamente
				- implementazione
					- nella tabella delle pagine, si utilizza un bit $v$ che indica se la pagina è presente nella memoria centrale (in un qualche frame) oppure no
					- nel caso in cui il processo tenti di accedere a una pagina non in memoria:
						1. il TLB miss (nel caso ci sia TLB) diventa un _page fault_
						2. un componente del SO, chiamato _pager_, si occupa di caricare la pagina mancante in memoria, e di aggiornare di conseguenza la tabella delle pagine
		- swap vs. demand paging
			- swap --> prendere l'intera memoria del processo e metterlo su memoria secondaria, per riattivarlo in un secondo momento;
			- demand paging --> potrebbe essere pensata come una tecnica di swap lazy, ossia viene caricato solo ciò che serve;
			- la confusione sta nel fatto che la swap area è dove il pager fa lo scambio tra dati in memoria primaria e secondaria --> dovrebbe essere chiamata paging area
		- gestione del page fault
			- in mancanza di frame liberi si verifica un page fault
			- algoritmo di demand paging
				1. individua pagina in memoria secondaria
				2. individua frame libero
				3. se non esiste frame libero
					1. si richiama algoritmo di rimpiazzamento
					2. aggiorna la tabella delle pagine (invalidando la pagina "vittima")
					3. se la pagina "vittima" è stata variata, scrive la pagina sul disco
					4. aggiorna la tabella dei frame (frame libero)
				4. aggiorna la tabella dei frame (frame occupato)
				5. leggi la pagina da disco (quella che ha provocato il fault)
				6. aggiorna la tabella delle pagine (caricato il TLB)
				7. riattiva il processo
			- l'obiettivo dell'algoritmo di rimpiazzamento è di minimizzare il numero di page fault
				- valutazione: si prende la _stringa di riferimenti_ in memoria
					- tale stringa alla fine è la stringa del numero di pagina --> ci interessa sapere la sequenza dei numeri di pagina acceduti
					- può essere generata randomicamente
				- ci si aspetta che più la memoria è grande, minore sarà il numero di page fault --> non è vero che aggiungendo memoria ci siano meno page fault
					- questa anomalia si chiama **anomalia di Belady**
				- algoritmi ([[Algoritmi di paginazione]])
					- FIFO
						- butta la pagina caricata per prima
						- semplice, ma il fatto che la pagina sia lì da tanto tempo non indica quanto sia utile o meno
						- "i vecchi talvolta servono"
					- MIN
						- butta la pagina che non sarà più acceduta o la pagina che verrà acceduta nel futuro più lontano
						- è ottimale, dimostrabile
						- ma irrealizzabile perché dovremmo prevedere la stringa dei riferimenti dei processi!
						- non è da buttare: dà un bound minimo, con cui fare il confronto con gli altri algoritmi
					- LRU
						- seleziona come pagina quella che è stata usata meno recentemente nel passato
						- il problema è che è necessario uno specifico supporto hardware: dobbiamo registrare nella tabella delle pagine un timestamp ogni volta che si accede a una pagina
							- questo timestamp può semplicemente essere un contatore che viene incrementato ad ogni accesso in memoria
							- ciò dev'essere fatto dall'MMU
							- ma questo contatore dovrebbe essere almeno a 128 bit per poter sostenere un upperbound del numero di accessi in memoria!
						- implementazione
							- a stack
								- si mantiene uno stack di pagine, e tutte le volte che una pagina viene acceduta torna in cima
								- ma è molto pesante
								- LRU è a stack
								- _quando un algoritmo si dice a stack_?
									- data la stringa di riferimenti $s$
									- dato l'algoritmo $A$
									- dato il tempo $t$
									- e data la dimensione della memoria di $m$ frame
									- dato $S_{t}(s, A, m)$ l'insieme delle pagine mantenute in memoria primaria al tempo $t$ dell'algoritmo $A$, data una memoria di $m$ frame relativamente alla stringa $s$
									- allora un algoritmo a stack si ha se per ogni istante $t$ e per ogni stringa $s$ $$S_{t}(s, A, m) \subseteq S_{t}(s, A, m+1)$$
									- ossia: l'insieme delle pagine in memoria con $m$ frame è sempre un sottoinsieme delle pagine in memoria con $m+1$ frame
									- teorema: un algoritmo a stack non genera casi di anomalia di Belady --> se aumento i frame non può aumentare il numero di page fault
										- prendiamo un qualsiasi $A, s, t, m$
										- assumiamo $A$ sia a stack, ovvero $S_{t}(s, A, m) \subseteq S_{t}(s, A, m+1)$
										- allora $S_{t}(s, A, m+1)$ contiene un elemento in più rispetto a $S_{t}(s, A, m)$
										- il riferimento in $s$ per l'istante $t+1$ può
											- se $p \in S_{t}(s, A, m)$ non c'è page fault
											- se $p \notin S_{t}(s, A, m)$
											- e se $p \in S_{t}(s, A, m+1)$ non c'è page fault
											- se $p \notin S_{t}(s, A, m+1)$
											- dalla relazione del sottoinsieme, abbiamo 3 casi
												- che $p \in S_{t}(s, A, m)$, allora $p \in S_{t}(s, A, m+1)$ --> non c'è mai page fault
												- se $p \notin S_{t}(s, A, m)$ e $p \in S_{t}(s, A, m+1$, allora c'è page fault solo nel caso $m$
												- se $p$ non c'è da entrambi, allora c'è page fault su entrambi
											- quindi in ogni caso, con $m+1$ frame non ci può essere un numero di page fault superiore al caso con $m$
						- fatto sta che non è usato, perché costerebbe troppo
					- approssimazione LRU
						- con un reference bit, un bit che va a 1 se si ha fatto l'accesso alla pagina
						- all'inizio ogni pagina ha il reference bit a 0; quando vengono accedute mettono il bit a 1
						- non sappiamo quali sono state accedute per prime e quali per ultime --> ma usiamo questo bit per approssimare l'algoritmo LRU, uniformando i tempi di accesso al solo concetto di accesso!
						- metodo: _additional-reference-bit-algorithm_
							- salviamo i reference bit a intervalli regolari
							- manteniamo in ogni pagina una "storia" di reference bit, tipo 8 bit
							- il nuovo valore del bit viene quindi salvato tramite shift a destra della storia, e inserimento del bit come più significativo --> l'ultimo accesso è quello più significativo
							- la pagina vittima è quella con valore degli 8 bit minore!

## Domande

## Referenze
