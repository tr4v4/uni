---
tags:
  - category/lecture
  - status/finished
  - topic/algebra-e-geometria
date: 12-03-2024 09:18:25
teacher: marta.morigi@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- prima di fare le applicazioni lineari --> esercizio in più per distinguere tra i metodi risolutivi studiati
	- _algoritmo di Gauss per risolvere sistemi lineari_
	- _algoritmo di Gauss in modo diretto_
	- esercizio
		- stabilire per quali valori di $k$ il vettore $v = \begin{pmatrix} 2 & k \\ k^{2} & 2k+4 \end{pmatrix} \in \langle \begin{pmatrix} 1 & 0 \\ 1 & 3 \end{pmatrix}, \begin{pmatrix} 1 & -1 \\ 0 & 2 \end{pmatrix}, \begin{pmatrix} 1 & -3 \\ 2 & 0 \end{pmatrix} \rangle$
			- 1° metodo, usando la definizione si ha che $v \in v_{1}, v_{2}, v_{3} \iff \exists \lambda_{1}, \lambda_{2}, \lambda_{3} \in \mathbb{R} : v = \lambda_{1}v_{1} + \lambda_{2}v_{2} + \lambda_{3}v_{3}$
				- cerchiamo questi $\lambda_{1}, \lambda_{2}, \lambda_{3}$
				- ci viene che $\exists$ soluzione $\iff k = 2$
			- 2° metodo, usando Gauss in modo diretto
				- ci permette di trovare una base del sottospazio generato dai tre vettori generatori
				- osservazione: sarebbe un errore dire che $v \in \langle v_{1}, v_{2}, v_{3} \rangle \iff v_{1}, v_{2}, v_{3}, v$ sono linearmente dipendenti; ciò che invece è vero è $v \in \langle v_{1}, v_{2}, v_{3} \rangle \implies v_{1}, v_{2}, v_{3}, v$ sono dipendenti, per cui il viceversa non è detto
					- questo perché la teoria ci dice che _uno_ di essi è combinazione lineare degli altri --> non è detto che sia proprio $v$
					- perciò Gauss in modo super diretto non funziona
				- l'idea è allora è intanto di _passare alle coordinate_ (rispetto alla base canonica)
					- questo perché c'è un isomorfismo tra matrici e vettori --> applicazione lineare
					- allora si ha $(v)_{\beta} = (2, k, k^{2}, 2k+4), (v_{1})_{\beta} = (1, 0, 1, 3), (v_{2})_{\beta} = (1, -1, 0, 2), (v_{3})_{\beta} = (1, -3, -2, 0)$
					- _usiamo Gauss in modo diretto per trovare una base di $V$_, scoprendo che $\dim(V) = 2$
					- una volta trovata ci aggiungiamo il vettore $(v)_{e}$ (siamo sempre nel dominio delle coordinate)
					- risolviamo e troviamo le stesse soluzioni di prima
						- anche in questo caso infatti se $k = 2$ abbiamo l'ultima riga nulla
						- questo vuol dire che il sottospazio generato da $\langle (v_{1})_{e}, (v_{2})_{e}, (v)_{e} \rangle$ ha la stessa dimensione di $\langle (v_{1})_{e}, (v_{2})_{e} \rangle$, per cui $\langle (v_{1})_{e}, (v_{2})_{e}, (v)_{e} \rangle = \langle (v_{1})_{e}, (v_{2})_{e} \rangle$, e allora (per proposizione dei sottospazi generati già fatta) si ha $(v)_{e} = \langle (v_{1})_{e}, (v_{2})_{e} \rangle$
						- se invece $k \neq 2$ si ha che il sottospazio ha una dimensione aumentata, per cui $(v)_{e} \notin \langle (v_{1})_{e}, (v_{2})_{e} \rangle \implies v \notin V$
					- fondamentalmente: se si utilizza Gauss in modo super diretto si trova una base del sottospazio generato da $(v, v_{1}, v_{2}, v_{3})$, e non sai se $v$ è combinazione lineare di $v_{1}, v_{2}, v_{3}$
- [[Applicazione lineare]]
	- sono quelle funzioni che rispettano la struttura di spazio vettoriale
	- definizione: siano $V, W$ spazi vettoriali, una funzione $f: V \to W$ se verificano le seguenti proprietà
		1. $f(v + u) = f(v) + f(u) \ \ \ \forall u, v \in V$ (morfismo di gruppi)
		2. $f(\lambda v) = \lambda f(v) \ \ \ \forall \lambda \in \mathbb{R}, \forall v \in V$
		- fondamentalmente devono essere rispettate la somma e il prodotto per scalare
	- esempi
		- dato $V$ spazio vettoriale e $\beta = \{v_{1}, \cdots, v_{n}\}$ base di $V$, si ha $f: V \to \mathbb{R}^{n} : v \to (v)_{\beta}$ (la funzione che associa a un vettore le sue coordinate rispetto a una base dello spazio a cui appartiene) è un'applicazione lineare
		- dati $V = \{f: \mathbb{R} \to \mathbb{R}\}$ derivabili, $W = \{f: \mathbb{R} \to \mathbb{R}\}$, allora $D: V \to W : f \to f'$ è un'applicazione lineare, infatti
			- $(f_{1} + f_{2})' = f_{1}' + f_{2}'$
			- $(\lambda f)' = \lambda f'$
		- data $f: \mathbb{R}^{2} \to \mathbb{R}^{3} : (x_{1}, x_{2}) \to (x_{1}, 2x_{2}, -x_{1})$, verifichiamo la linearità
			- somma
				- $v = (a_{1}, a_{2}), u = (b_{1}, b_{2})$, allora $v+u = (a_{1}+b_{1}, a_{2}+b_{2})$
				- $f(v+u) = f(a_{1}+b_{1}, a_{2}+b_{2}) = (a_{1}+b_{1}, 2(a_{2}+b_{2}), -(a_{1}+b_{1})) = (a_{1}+b_{1}, 2a_{2}+2b_{2}, -a_{1}-b_{1})$
				- $f(v) + f(u) = (a_{1}, 2a_{2}, -a_{1}) + (b_{1}, 2b_{2}, -b_{1})$
				- si ha $f(v+u) = f(v) + f(u)$
			- prodotto per scalari
				- $v = (a_{1}, b_{1}), \lambda \in \mathbb{R}$
				- $f(\lambda v) = f(\lambda a_{1}, \lambda a_{2}) = (\lambda a_{1}, 2 \lambda a_{2}, - \lambda a_{1}) = \lambda (a_{1}, 2a_{2}, -a_{1}) = \lambda f(v)$
		- data $f: \mathbb{R}^{3} \to \mathbb{R}^{2} : (x_{1}, x_{2}, x_{3}) \to (3x_{2}, -x_{1}+x_{3}+1)$ non è un'applicazione lineare
		- data la matrice $A \in M_{m \times n} (\mathbb{R})$, allora $f: \mathbb{R}^{n} \to \mathbb{R}^{m} : x=(x_{1}, \cdots, x_{n}) \to A \cdot \begin{pmatrix} x_{1} \\ \vdots \\ x_{n} \end{pmatrix}$ è lineare
			- somma
				- $f(\underline{x} + \underline{y}) = A \cdot (\underline{x} + \underline{y}) = A\underline{x} + A\underline{y} = f(\underline{x}) + f(\underline{y})$
			- prodotto per scalari
				- $f(\lambda\underline{x}) = A(\lambda\underline{x}) = \lambda A\underline{x} = \lambda f(\underline{x})$
				- osservazione
					- presa $A = \begin{pmatrix} 3 & -1 & 5 \\ 0 & 2 & 1 \end{pmatrix}$ allora $f: \mathbb{R}^{3} \to \mathbb{R}^{2}$ e $f(x_{1}, x_{2}, x_{3}) = \begin{pmatrix} 3 & -1 & 5 \\ 0 & 2 & 1 \end{pmatrix} \cdot \begin{pmatrix} x_{1} \\ x_{2} \\ x_{3} \end{pmatrix}$
					- questo è importantissimo, perché è una funzione che presi $(x_{1}, x_{2}, x_{3})$ canoniche, ovvero $(1, 0, 0), (0, 1, 0), (0, 0, 1)$, dà come risultati le colonne di $A$ --> succede sempre
						- la prima colonna è l'immagine del primo vettore della base canonica; la seconda colonna è l'immagine del secondo vettore della base canonica; e così via...
	- si chiama applicazione lineare, o funzione lineare o mappa lineare

## Domande

## Referenze
