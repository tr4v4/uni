---
tags:
  - category/lecture
  - status/finished
  - topic/introduzione-all-apprendimento-automatico
date: 09-10-2025 13:13:47
teacher: andrea.asperti@unibo.it
mod: 1
---
# Lezione
---
## Concetti
- **distribuzione congiunta**
	- costruiamo una tabella con tutte le possibili combinazioni dei valori delle features, e ad ogni combinazione stimiamo la probabilità
	- date $n$ features booleane dobbiamo stimare $2^{n}-1$ parametri
		- l'ultima è il complemento!
	- questa si chiama distribuzione congiunta delle variabili aleatorie
	- è letteralmente la densità congiunta discreta
	- ovviamente, all'aumentare delle features diventa complesso stimare le probabilità
	- perché è comunque utile?
		- data la distribuzione congiunta possiamo stimare la probabilità di qualunque evento data come combinazione logica delle features!
		- $\mathbb{P}(E) = \sum\limits_{row \in E}{\mathbb{P}(row)}$
		- si può facilmente calcolare anche la probabilità condizionata
	- il nostro obiettivo era quello di calcolare, invece che $f: X \to Y$, la probabilità condizionata $p: P(Y|X)$
		- e la distribuzione congiunta ci consente di fare questo
	- per ottenere valori significativi avremmo bisogno di molti esempi per ogni configurazione
	- esempio con classificazione delle immagini con mnist --> con immagini 28x28 avremmo $2^{784}$ righe, quindi teoricamente è fattibile ma nella pratica assolutamente no
- **classificatori bayesiani**
	- possiamo sfruttare la regola di bayes? Sì, ma comunque servono $2^{n-1}$ stime...
- **naive Bayes**
	- per questo assumiamo che $\mathbb{P}(X_{1}, \cdots, X_{n} | Y) = \prod_{i}\mathbb{P}(X_{i}|Y)$, ossia che $X_{1}, \cdots, X_{n}$ siano indipendenti tra di loro, dato $Y$
	- in questo modo facilitiamo i calcoli! Da esponenziale diventa lineare il numero di parametri --> $2n$
	- diventa un'approssimazione della distribuzione congiunta!
	- formula di classificazione di un nuovo dato
	- adesso dobbiamo capire come stimare le probabilità!
		- dobbiamo stimare $\pi_{k}$ e $\theta_{ijk}$
		- usiamo la MLE (Maximum Likelihood Estimates)
			- è la cosa ovvia
				- $\pi_{k} = \mathbb{P}(Y = y_{k}) = \frac{\#\mathcal{D}\{Y=y_{k}\}}{|\mathcal{D}|}$ guardo quanti elementi ho nelle classi e lo rapporto a tutti gli elementi
				- $\theta_{ijk} = \mathbb{P}(X_{i} = x_{ij} | Y = y_{k})$
	- nota bene: i risultati della predizione non sono probabilità, devono essere normalizzati dividendo per $\mathbb{P}(X_{1}, \cdots, X_{n})$, ossia la somma delle predizioni calcolate (di cui si prende il massimo)
	- natura generativa
	- problematiche
		- problema con probabilità pari a 0
		- la supposizione di indipendenza
		- la linearità --> non riesce a catturare anche classificazioni semplici
			- guarda 1 solo pixel alla volta

## Domande

## Referenze
