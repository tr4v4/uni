
"smart_sources:2. notes/Teoria dell'informazione.md": {"path":"2. notes/Teoria dell'informazione.md","last_embed":{"hash":null},"embeddings":{},"last_read":{"hash":"kwyq7v","at":1760976449559},"class_name":"SmartSource","last_import":{"mtime":1759389487444,"size":2080,"at":1760976449564,"hash":"kwyq7v"},"blocks":{"#---frontmatter---":[1,9],"#Teoria dell'informazione":[10,32],"#Teoria dell'informazione#{1}":[11,11],"#Teoria dell'informazione#Introduzione":[12,16],"#Teoria dell'informazione#Introduzione#{1}":[13,16],"#Teoria dell'informazione#Interpretazione":[17,31],"#Teoria dell'informazione#Interpretazione#Entropia":[18,31],"#Teoria dell'informazione#Interpretazione#Entropia#{1}":[19,19],"#Teoria dell'informazione#Interpretazione#Entropia#{2}":[20,20],"#Teoria dell'informazione#Interpretazione#Entropia#{3}":[21,21],"#Teoria dell'informazione#Interpretazione#Entropia#{4}":[22,23],"#Teoria dell'informazione#Interpretazione#Entropia#{5}":[24,31],"#Teoria dell'informazione#Referenze":[32,32]},"outlinks":[{"title":"Lecture 26092025131224","target":"Lecture 26092025131224","line":8},{"title":"entropia","target":"Entropia","line":14},{"title":"guadagno informativo","target":"Guadagno informativo","line":15},{"title":"probabilita'","target":"Probabilita'","line":19},{"title":"eventi indipendenti","target":"Eventi indipendenti","line":21},{"title":"codifica di Huffman","target":"Codifica di Huffman","line":30},{"title":"distribuzione uniforme","target":"Distribuzione uniforme discreta","line":30}],"metadata":{"tags":["#category/note","#status/finished","#topic/introduzione-all-apprendimento-automatico"],"date":"01-10-2025 19:38:20","links":["[[Lecture 26092025131224]]"]},"task_lines":[]},